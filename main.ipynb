{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "qvFcIoG0-on3",
        "CNZwJqrH1U7p",
        "VAY_XDN2JWWM",
        "FXpQWy9oAnzP",
        "00yS-DjrACRG",
        "C25jDbmYM5ay",
        "QJPwGil_AcHF",
        "dhoPcxWiELlY",
        "q7nc6IprDvId",
        "cK7514kUD9CD",
        "7Kllf_4yEQDs",
        "wA-v01srJdKV",
        "9Vmvh5-9DoId",
        "ddgY-6YeDp2F",
        "m1vsc1587kBG",
        "Fr2_HoBC7pVT",
        "oAFd11kU8_vJ",
        "Vy2o1KSq94BN",
        "9r8gmo1j98DL",
        "r2DWiWYeP1I2",
        "sRPOoeLkP1I6",
        "_ud7vmZ_P1JY",
        "KuksYQGo--y9",
        "b5v8YzTq_Bwd",
        "quHne06m_RlU",
        "1m7Q3mGhUysb",
        "_ViGUUZVGIy2",
        "hdmn8AKGnbfx",
        "xqf_fvdEHNC5",
        "LEretH0LQbM0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickvanhulst/long-short-term-dependencies-recommender-systems/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9k1GGjU2YHD",
        "colab_type": "text"
      },
      "source": [
        "# Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNfQoXVCy-XJ",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains the code for the work performed by J.M. van Hulst for his thesis. Using this Notebook is fairly easy, one only has to alter the variable names below to the name of the **Google Drive** folder where the experiments should be stored (**NOTE**: make sure that the variable name does not end with a backslash) and the **folder** where the LastFM dataset is stored. The LastFM dataset is freely available [here](https://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html). We encourage users to run this notebook on Google Colab as the models are computationally expensive.\n",
        "\n",
        "To obtain results for this work, one only has to run all the cells, which can be done by pressing CTRL+F9 (or click on Runtime above). After which, you may freely grab a cup of coffee and enjoy the ride.\n",
        "\n",
        "The code for the DNC was largely based on the work performed by Csordas et al., his code can be found [here](https://github.com/xdever/dnc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzMqvMmcbKw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOME_FOLDER = '' # Example: /content/gdrive/My Drive/projects/rec_system\n",
        "LASTFM_STORAGE_FOLDER = '' # Example: '/content/gdrive/My Drive/raw_data/lastfm_1k/main.tsv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvFcIoG0-on3",
        "colab_type": "text"
      },
      "source": [
        "# Packages & Static Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcBStLrd-qJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH_SEQUENCE = 20\n",
        "MIN_THRESH = 20\n",
        "\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "import os, sys\n",
        "\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lu4o3qtJ8Qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import time\n",
        "import sys\n",
        "import signal\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import copy\n",
        "import functools\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.nn import Module, Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "RANDOM_SEEDS = list(range(RANDOM_SEED, RANDOM_SEED + 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pww_ewoayaI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(HOME_FOLDER):\n",
        "    os.mkdir(HOME_FOLDER)\n",
        "    \n",
        "for exp in ['/exp1/', '/exp2/', '/exp3/', '/exp4/']:\n",
        "    if not os.path.isdir(HOME_FOLDER + exp):\n",
        "        for f in ['', 'models']:\n",
        "            os.mkdir(HOME_FOLDER + exp + f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNZwJqrH1U7p",
        "colab_type": "text"
      },
      "source": [
        "# Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAY_XDN2JWWM",
        "colab_type": "text"
      },
      "source": [
        "## DNC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXpQWy9oAnzP",
        "colab_type": "text"
      },
      "source": [
        "### Remainder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CdSpLTT-vEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2017 Robert Csordas. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "def oneplus(t):\n",
        "    return F.softplus(t, 1, 20) + 1.0\n",
        "\n",
        "def get_next_tensor_part(src, dims, prev_pos=0):\n",
        "    if not isinstance(dims, list):\n",
        "        dims=[dims]\n",
        "    n = functools.reduce(lambda x, y: x * y, dims)\n",
        "    data = src.narrow(-1, prev_pos, n)\n",
        "    return data.contiguous().view(list(data.size())[:-1] + dims) if len(dims)>1 else data, prev_pos + n\n",
        "\n",
        "def split_tensor(src, shapes):\n",
        "    pos = 0\n",
        "    res = []\n",
        "    for s in shapes:\n",
        "        d, pos = get_next_tensor_part(src, s, pos)\n",
        "        res.append(d)\n",
        "    return res\n",
        "\n",
        "def dict_get(dict,name):\n",
        "    return dict.get(name) if dict is not None else None\n",
        "\n",
        "\n",
        "def dict_append(dict, name, val):\n",
        "    if dict is not None:\n",
        "        l = dict.get(name)\n",
        "        if not l:\n",
        "            l = []\n",
        "            dict[name] = l\n",
        "        l.append(val)\n",
        "\n",
        "\n",
        "def init_debug(debug, initial):\n",
        "    if debug is not None and not debug:\n",
        "        debug.update(initial)\n",
        "\n",
        "def merge_debug_tensors(d, dim):\n",
        "    if d is not None:\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, dict):\n",
        "                merge_debug_tensors(v, dim)\n",
        "            elif isinstance(v, list):\n",
        "                d[k] = torch.stack(v, dim)\n",
        "\n",
        "\n",
        "def linear_reset(module, gain=1.0):\n",
        "    assert isinstance(module, torch.nn.Linear)\n",
        "    init.xavier_uniform_(module.weight, gain=gain)\n",
        "    s = module.weight.size(1)\n",
        "    if module.bias is not None:\n",
        "        module.bias.data.zero_()\n",
        "\n",
        "_EPS = 1e-6\n",
        "\n",
        "class AllocationManager(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AllocationManager, self).__init__()\n",
        "        self.usages = None\n",
        "        self.zero_usages = None\n",
        "        self.debug_sequ_init = False\n",
        "        self.one = None\n",
        "\n",
        "    def _init_sequence(self, prev_read_distributions):\n",
        "        # prev_read_distributions size is [batch, n_heads, cell count]\n",
        "        s = prev_read_distributions.size()\n",
        "        if self.zero_usages is None or list(self.zero_usages.size())!=[s[0],s[-1]]:\n",
        "            self.zero_usages = torch.zeros(s[0], s[-1], device = prev_read_distributions.device)\n",
        "            if self.debug_sequ_init:\n",
        "                self.zero_usages += torch.arange(0, s[-1]).unsqueeze(0) * 1e-10\n",
        "\n",
        "        self.usages = self.zero_usages\n",
        "\n",
        "    def _init_consts(self, device):\n",
        "        if self.one is None:\n",
        "            self.one = torch.ones(1, device=device)\n",
        "\n",
        "    def new_sequence(self):\n",
        "        self.usages = None\n",
        "\n",
        "    def update_usages(self, prev_write_distribution, prev_read_distributions, free_gates):\n",
        "        # Read distributions shape: [batch, n_heads, cell count]\n",
        "        # Free gates shape: [batch, n_heads]\n",
        "\n",
        "        self._init_consts(prev_read_distributions.device)\n",
        "        phi = torch.addcmul(self.one, -1, free_gates.unsqueeze(-1), prev_read_distributions).prod(-2)\n",
        "        # Phi is the free tensor, sized [batch, cell count]\n",
        "\n",
        "        # If memory usage counter if doesn't exists\n",
        "        if self.usages is None:\n",
        "            self._init_sequence(prev_read_distributions)\n",
        "            # in first timestep nothing is written or read yet, so we don't need any further processing\n",
        "        else:\n",
        "            self.usages = torch.addcmul(self.usages, 1, prev_write_distribution.detach(), (1 - self.usages)) * phi\n",
        "\n",
        "        return phi\n",
        "\n",
        "    def forward(self, prev_write_distribution, prev_read_distributions, free_gates):\n",
        "        phi = self.update_usages(prev_write_distribution, prev_read_distributions, free_gates)\n",
        "        sorted_usage, free_list = (self.usages*(1.0-_EPS)+_EPS).sort(-1)\n",
        "\n",
        "        u_prod = sorted_usage.cumprod(-1)\n",
        "        one_minus_usage = 1.0 - sorted_usage\n",
        "        sorted_scores = torch.cat([one_minus_usage[..., 0:1], one_minus_usage[..., 1:] * u_prod[..., :-1]], dim=-1)\n",
        "\n",
        "        return sorted_scores.clone().scatter_(-1, free_list, sorted_scores), phi\n",
        "\n",
        "\n",
        "class ContentAddressGenerator(torch.nn.Module):\n",
        "    def __init__(self, disable_content_norm=False, mask_min=0.0, disable_key_masking=False):\n",
        "        super(ContentAddressGenerator, self).__init__()\n",
        "        self.disable_content_norm = disable_content_norm\n",
        "        self.mask_min = mask_min\n",
        "        self.disable_key_masking = disable_key_masking\n",
        "\n",
        "    def forward(self, memory, keys, betas, mask=None):\n",
        "        # Memory shape [batch, cell count, word length]\n",
        "        # Key shape [batch, n heads*, word length]\n",
        "        # Betas shape [batch, n heads]\n",
        "        if mask is not None and self.mask_min != 0:\n",
        "            mask = mask * (1.0-self.mask_min) + self.mask_min\n",
        "\n",
        "        single_head = keys.dim() == 2\n",
        "        if single_head:\n",
        "            # Single head\n",
        "            keys = keys.unsqueeze(1)\n",
        "            if mask is not None:\n",
        "                mask = mask.unsqueeze(1)\n",
        "\n",
        "        memory = memory.unsqueeze(1)\n",
        "        keys = keys.unsqueeze(-2)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(-2)\n",
        "            memory = memory * mask\n",
        "            if not self.disable_key_masking:\n",
        "                keys = keys * mask\n",
        "\n",
        "        # Shape [batch, n heads, cell count]\n",
        "        norm = keys.norm(dim=-1)\n",
        "        if not self.disable_content_norm:\n",
        "            norm = norm * memory.norm(dim=-1)\n",
        "\n",
        "        scores = (memory * keys).sum(-1) / (norm + _EPS)\n",
        "        scores *= betas.unsqueeze(-1)\n",
        "\n",
        "        res = F.softmax(scores, scores.dim()-1)\n",
        "        return res.squeeze(1) if single_head else res\n",
        "\n",
        "\n",
        "class WriteHead(torch.nn.Module):\n",
        "    @staticmethod\n",
        "    def create_write_archive(write_dist, erase_vector, write_vector, phi):\n",
        "        return dict(write_dist=write_dist, erase_vector=erase_vector, write_vector=write_vector, phi=phi)\n",
        "\n",
        "    def __init__(self, dealloc_content=True, disable_content_norm=False, mask_min=0.0, disable_key_masking=False):\n",
        "        super(WriteHead, self).__init__()\n",
        "        self.write_content_generator = ContentAddressGenerator(disable_content_norm, mask_min=mask_min, disable_key_masking=disable_key_masking)\n",
        "        self.allocation_manager = AllocationManager()\n",
        "        self.last_write = None\n",
        "        self.dealloc_content = dealloc_content\n",
        "        self.new_sequence()\n",
        "\n",
        "    def new_sequence(self):\n",
        "        self.last_write = None\n",
        "        self.allocation_manager.new_sequence()\n",
        "\n",
        "    @staticmethod\n",
        "    def mem_update(memory, write_dist, erase_vector, write_vector, phi):\n",
        "        # In original paper the memory content is NOT deallocated, which makes content based addressing basically\n",
        "        # unusable when multiple similar steps should be done. The reason for this is that the memory contents are\n",
        "        # still there, so the lookup will find them, unless an allocation clears it before the next search, which is\n",
        "        # completely random. So I'm arguing that erase matrix should also take in account the free gates (multiply it\n",
        "        # with phi)\n",
        "        write_dist = write_dist.unsqueeze(-1)\n",
        "\n",
        "        erase_matrix = 1.0 - write_dist * erase_vector.unsqueeze(-2)\n",
        "        if phi is not None:\n",
        "            erase_matrix = erase_matrix * phi.unsqueeze(-1)\n",
        "\n",
        "        update_matrix = write_dist * write_vector.unsqueeze(-2)\n",
        "        return memory * erase_matrix + update_matrix\n",
        "\n",
        "    def forward(self, memory, write_content_key, write_beta, erase_vector, write_vector, alloc_gate, write_gate,\n",
        "                free_gates, prev_read_dist, write_mask=None, debug=None):\n",
        "        last_w_dist = self.last_write[\"write_dist\"] if self.last_write is not None else None\n",
        "\n",
        "        content_dist = self.write_content_generator(memory, write_content_key, write_beta, mask = write_mask)\n",
        "        alloc_dist, phi = self.allocation_manager(last_w_dist, prev_read_dist, free_gates)\n",
        "\n",
        "        # Shape [batch, cell count]\n",
        "        write_dist = write_gate * (alloc_gate * alloc_dist + (1-alloc_gate)*content_dist)\n",
        "        self.last_write = WriteHead.create_write_archive(write_dist, erase_vector, write_vector, phi if self.dealloc_content else None)\n",
        "\n",
        "        dict_append(debug, \"alloc_dist\", alloc_dist)\n",
        "        dict_append(debug, \"write_dist\", write_dist)\n",
        "        dict_append(debug, \"mem_usages\", self.allocation_manager.usages)\n",
        "        dict_append(debug, \"free_gates\", free_gates)\n",
        "        dict_append(debug, \"write_betas\", write_beta)\n",
        "        dict_append(debug, \"write_gate\", write_gate)\n",
        "        dict_append(debug, \"write_vector\", write_vector)\n",
        "        dict_append(debug, \"alloc_gate\", alloc_gate)\n",
        "        dict_append(debug, \"erase_vector\", erase_vector)\n",
        "        if write_mask is not None:\n",
        "            dict_append(debug, \"write_mask\", write_mask)\n",
        "\n",
        "        return WriteHead.mem_update(memory, **self.last_write)\n",
        "\n",
        "class RawWriteHead(torch.nn.Module):\n",
        "    def __init__(self, n_read_heads, word_length, use_mask=False, dealloc_content=True, disable_content_norm=False,\n",
        "                 mask_min=0.0, disable_key_masking=False):\n",
        "        super(RawWriteHead, self).__init__()\n",
        "        self.write_head = WriteHead(dealloc_content = dealloc_content, disable_content_norm = disable_content_norm,\n",
        "                                    mask_min=mask_min, disable_key_masking=disable_key_masking)\n",
        "        self.word_length = word_length\n",
        "        self.n_read_heads = n_read_heads\n",
        "        self.use_mask = use_mask\n",
        "        self.input_size = 3*self.word_length + self.n_read_heads + 3 + (self.word_length if use_mask else 0)\n",
        "\n",
        "    def new_sequence(self):\n",
        "        self.write_head.new_sequence()\n",
        "\n",
        "    def get_prev_write(self):\n",
        "        return self.write_head.last_write\n",
        "\n",
        "    def forward(self, memory, nn_output, prev_read_dist, debug):\n",
        "        shapes = [[self.word_length]] * (4 if self.use_mask else 3) + [[self.n_read_heads]] + [[1]] * 3\n",
        "        tensors = split_tensor(nn_output, shapes)\n",
        "\n",
        "        if self.use_mask:\n",
        "            write_mask = torch.sigmoid(tensors[0])\n",
        "            tensors=tensors[1:]\n",
        "        else:\n",
        "            write_mask = None\n",
        "\n",
        "        write_content_key, erase_vector, write_vector, free_gates, write_beta, alloc_gate, write_gate = tensors\n",
        "\n",
        "        erase_vector = torch.sigmoid(erase_vector)\n",
        "        free_gates = torch.sigmoid(free_gates)\n",
        "        write_beta = oneplus(write_beta)\n",
        "        alloc_gate = torch.sigmoid(alloc_gate)\n",
        "        write_gate = torch.sigmoid(write_gate)\n",
        "\n",
        "        return self.write_head(memory, write_content_key, write_beta, erase_vector, write_vector,\n",
        "                               alloc_gate, write_gate, free_gates, prev_read_dist, debug=debug, write_mask=write_mask)\n",
        "\n",
        "    def get_neural_input_size(self):\n",
        "        return self.input_size\n",
        "\n",
        "\n",
        "class ReadHead(torch.nn.Module):\n",
        "    def __init__(self, disable_content_norm=False, mask_min=0.0, disable_key_masking=False):\n",
        "        super(ReadHead, self).__init__()\n",
        "        self.content_addr_generator = ContentAddressGenerator(disable_content_norm=disable_content_norm,\n",
        "                                                              mask_min=mask_min,\n",
        "                                                              disable_key_masking=disable_key_masking)\n",
        "        self.read_dist = None\n",
        "        self.read_data = None\n",
        "        self.new_sequence()\n",
        "\n",
        "    def new_sequence(self):\n",
        "        self.read_dist = None\n",
        "        self.read_data = None\n",
        "\n",
        "    def forward(self, memory, read_content_keys, read_betas, forward_dist, backward_dist, gates, read_mask=None, debug=None):\n",
        "        content_dist = self.content_addr_generator(memory, read_content_keys, read_betas, mask=read_mask)\n",
        "\n",
        "        self.read_dist = backward_dist * gates[..., 0:1] + content_dist * gates[...,1:2] + forward_dist * gates[..., 2:]\n",
        "\n",
        "        # memory shape: [ batch, cell count, word_length ]\n",
        "        # read_dist shape: [ batch, n heads, cell count ]\n",
        "        # result shape: [ batch, n_heads, word_length ]\n",
        "        self.read_data = (memory.unsqueeze(1) * self.read_dist.unsqueeze(-1)).sum(-2)\n",
        "\n",
        "        dict_append(debug, \"content_dist\", content_dist)\n",
        "        dict_append(debug, \"balance\", gates)\n",
        "        dict_append(debug, \"read_dist\", self.read_dist)\n",
        "        dict_append(debug, \"read_content_keys\", read_content_keys)\n",
        "        if read_mask is not None:\n",
        "            dict_append(debug, \"read_mask\", read_mask)\n",
        "        dict_append(debug, \"read_betas\", read_betas.unsqueeze(-2))\n",
        "        if read_mask is not None:\n",
        "            dict_append(debug, \"read_mask\", read_mask)\n",
        "\n",
        "        return self.read_data\n",
        "\n",
        "\n",
        "class RawReadHead(torch.nn.Module):\n",
        "    def __init__(self, n_heads, word_length, use_mask=False, disable_content_norm=False, mask_min=0.0,\n",
        "                 disable_key_masking=False):\n",
        "        super(RawReadHead, self).__init__()\n",
        "        self.read_head = ReadHead(disable_content_norm=disable_content_norm, mask_min=mask_min,\n",
        "                                  disable_key_masking=disable_key_masking)\n",
        "        self.n_heads = n_heads\n",
        "        self.word_length = word_length\n",
        "        self.use_mask = use_mask\n",
        "        self.input_size = self.n_heads * (self.word_length*(2 if use_mask else 1) + 3 + 1)\n",
        "\n",
        "    def get_prev_dist(self, memory):\n",
        "        if self.read_head.read_dist is not None:\n",
        "            return self.read_head.read_dist\n",
        "        else:\n",
        "            m_shape = memory.size()\n",
        "            return torch.zeros(m_shape[0], self.n_heads, m_shape[1]).to(memory)\n",
        "\n",
        "    def get_prev_data(self, memory):\n",
        "        if self.read_head.read_data is not None:\n",
        "            return self.read_head.read_data\n",
        "        else:\n",
        "            m_shape = memory.size()\n",
        "            return torch.zeros(m_shape[0], self.n_heads, m_shape[-1]).to(memory)\n",
        "\n",
        "    def new_sequence(self):\n",
        "        self.read_head.new_sequence()\n",
        "\n",
        "    def forward(self, memory, nn_output, forward_dist, backward_dist, debug):\n",
        "        shapes = [[self.n_heads, self.word_length]] * (2 if self.use_mask else 1) + [[self.n_heads], [self.n_heads, 3]]\n",
        "        tensors = split_tensor(nn_output, shapes)\n",
        "\n",
        "        if self.use_mask:\n",
        "            read_mask = torch.sigmoid(tensors[0])\n",
        "            tensors = tensors[1:]\n",
        "        else:\n",
        "            read_mask = None\n",
        "\n",
        "        keys, betas, gates = tensors\n",
        "\n",
        "        betas = oneplus(betas)\n",
        "        gates = F.softmax(gates, gates.dim()-1)\n",
        "\n",
        "        return self.read_head(memory, keys, betas, forward_dist, backward_dist, gates, debug=debug, read_mask=read_mask)\n",
        "\n",
        "    def get_neural_input_size(self):\n",
        "        return self.input_size\n",
        "\n",
        "class DistSharpnessEnhancer(torch.nn.Module):\n",
        "    def __init__(self, n_heads):\n",
        "        super(DistSharpnessEnhancer, self).__init__()\n",
        "        self.n_heads = n_heads if isinstance(n_heads, list) else [n_heads]\n",
        "        self.n_data = sum(self.n_heads)\n",
        "\n",
        "    def forward(self, nn_input, *dists):\n",
        "        assert len(dists) == len(self.n_heads)\n",
        "        nn_input = oneplus(nn_input[..., :self.n_data])\n",
        "        factors = split_tensor(nn_input, self.n_heads)\n",
        "\n",
        "        res = []\n",
        "        for i, d in enumerate(dists):\n",
        "            s = list(d.size())\n",
        "            ndim = d.dim()\n",
        "            f  = factors[i]\n",
        "            if ndim==2:\n",
        "                assert self.n_heads[i]==1\n",
        "            elif ndim==3:\n",
        "                f = f.unsqueeze(-1)\n",
        "            else:\n",
        "                assert False\n",
        "\n",
        "            d += _EPS\n",
        "            d = d / d.max(dim=-1, keepdim=True)[0]\n",
        "            d = d.pow(f)\n",
        "            d = d / d.sum(dim=-1, keepdim=True)\n",
        "            res.append(d)\n",
        "        return res\n",
        "\n",
        "    def get_neural_input_size(self):\n",
        "        return self.n_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00yS-DjrACRG",
        "colab_type": "text"
      },
      "source": [
        "### Temporal Linkage Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmlyuEJTVUCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TemporalMemoryLinkage(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TemporalMemoryLinkage, self).__init__()\n",
        "        self.temp_link_mat = None\n",
        "        self.precedence_weighting = None\n",
        "        self.diag_mask = None\n",
        "\n",
        "        self.initial_temp_link_mat = None\n",
        "        self.initial_precedence_weighting = None\n",
        "        self.initial_diag_mask = None\n",
        "        self.initial_shape = None\n",
        "\n",
        "    def new_sequence(self):\n",
        "        self.temp_link_mat = None\n",
        "        self.precedence_weighting = None\n",
        "        self.diag_mask = None\n",
        "\n",
        "    def _init_link(self, w_dist):\n",
        "        s = list(w_dist.size())\n",
        "        if self.initial_shape is None or s != self.initial_shape:\n",
        "            self.initial_temp_link_mat = torch.zeros(s[0], s[-1], s[-1]).to(w_dist.device)\n",
        "            self.initial_precedence_weighting = torch.zeros(s[0], s[-1]).to(w_dist.device)\n",
        "            self.initial_diag_mask = (1.0 - torch.eye(s[-1]).unsqueeze(0).to(w_dist)).detach()\n",
        "\n",
        "        self.temp_link_mat = self.initial_temp_link_mat\n",
        "        self.precedence_weighting = self.initial_precedence_weighting\n",
        "        self.diag_mask = self.initial_diag_mask\n",
        "\n",
        "    def _update_precedence(self, w_dist):\n",
        "        # w_dist shape: [ batch, cell count ]\n",
        "        self.precedence_weighting = (1.0 - w_dist.sum(-1, keepdim=True)) * self.precedence_weighting + w_dist\n",
        "\n",
        "    def _update_links(self, w_dist):\n",
        "        if self.temp_link_mat is None:\n",
        "            self._init_link(w_dist)\n",
        "\n",
        "        wt_i = w_dist.unsqueeze(-1)\n",
        "        wt_j = w_dist.unsqueeze(-2)\n",
        "        pt_j = self.precedence_weighting.unsqueeze(-2)\n",
        "\n",
        "        self.temp_link_mat = ((1 - wt_i - wt_j) * self.temp_link_mat + wt_i * pt_j) * self.diag_mask\n",
        "\n",
        "    def forward(self, w_dist, prev_r_dists, debug = None):\n",
        "        self._update_links(w_dist)\n",
        "        self._update_precedence(w_dist)\n",
        "\n",
        "        # Emulate matrix-vector multiplication by broadcast and sum. This way we don't need to transpose the matrix\n",
        "        tlm_multi_head = self.temp_link_mat.unsqueeze(1)\n",
        "\n",
        "        forward_dist = (tlm_multi_head * prev_r_dists.unsqueeze(-2)).sum(-1)\n",
        "        backward_dist = (tlm_multi_head * prev_r_dists.unsqueeze(-1)).sum(-2)\n",
        "        \n",
        "        dict_append(debug, \"forward_dists\", forward_dist)\n",
        "        dict_append(debug, \"backward_dists\", backward_dist)\n",
        "        dict_append(debug, \"precedence_weights\", self.precedence_weighting)\n",
        "\n",
        "        # output shapes [ batch, n_heads, cell_count ]\n",
        "        return forward_dist, backward_dist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSMvhUTrAObD",
        "colab_type": "text"
      },
      "source": [
        "### DNC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJTlibjoANsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DNC(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size, word_length, cell_count, n_read_heads, controller, batch_first=False, clip_controller=20,\n",
        "                 bias=True, mask=False, dealloc_content=True, link_sharpness_control=True, disable_content_norm=False,\n",
        "                 mask_min=0.0, disable_key_masking=False, output_classes=43, use_temp_link_gradient=False, seq_len=20,\n",
        "                return_sequences=False):\n",
        "        super(DNC, self).__init__()\n",
        "\n",
        "        self.clip_controller = clip_controller\n",
        "\n",
        "        self.read_head = RawReadHead(n_read_heads, word_length, use_mask=mask, disable_content_norm=disable_content_norm,\n",
        "                                     mask_min=mask_min, disable_key_masking=disable_key_masking)\n",
        "        self.write_head = RawWriteHead(n_read_heads, word_length, use_mask=mask, dealloc_content=dealloc_content,\n",
        "                                       disable_content_norm=disable_content_norm, mask_min=mask_min,\n",
        "                                       disable_key_masking=disable_key_masking)\n",
        "        self.temporal_link = TemporalMemoryLinkage()\n",
        "        #self.temp_test = TemporalMemoryLinkageSparse()\n",
        "        #NOTE: added myself\n",
        "        self.use_temp_link_gradient = use_temp_link_gradient\n",
        "        self.sharpness_control = DistSharpnessEnhancer([n_read_heads, n_read_heads]) if link_sharpness_control else None\n",
        "\n",
        "        in_size = input_size + n_read_heads * word_length\n",
        "        control_channels = self.read_head.get_neural_input_size() + self.write_head.get_neural_input_size() +\\\n",
        "                           (self.sharpness_control.get_neural_input_size() if self.sharpness_control is not None else 0)\n",
        "\n",
        "        self.controller = controller\n",
        "        controller.init(in_size)\n",
        "        \n",
        "        #NOTE: Original paper has shared biases, this does not.\n",
        "        self.controller_to_controls = torch.nn.Linear(controller.get_output_size(), control_channels, bias=bias)\n",
        "        self.controller_to_out = torch.nn.Linear(controller.get_output_size(), output_size, bias=bias)\n",
        "        self.read_to_out = torch.nn.Linear(word_length * n_read_heads, output_size, bias=bias)\n",
        "\n",
        "        #NOTE: Added last FCN, layer normalization and dropout (last two according to ADNC).\n",
        "        self.return_sequences = return_sequences\n",
        "        if self.return_sequences:\n",
        "            self.last_layer = torch.nn.Linear(output_size * seq_len, output_classes, bias=bias)\n",
        "        else:\n",
        "            self.last_layer = torch.nn.Linear(output_size, output_classes, bias=bias)\n",
        "        #self.layer_norm = torch.nn.LayerNorm(control_channels)\n",
        "#         self.drop_out = torch.nn.Dropout(p=0.2)\n",
        "        \n",
        "        self.cell_count = cell_count\n",
        "        self.word_length = word_length\n",
        "\n",
        "        self.memory = None\n",
        "        self.reset_parameters()\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "        self.zero_mem_tensor = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        linear_reset(self.controller_to_controls)\n",
        "        linear_reset(self.controller_to_out)\n",
        "        linear_reset(self.read_to_out)\n",
        "        linear_reset(self.last_layer)\n",
        "        self.controller.reset_parameters()\n",
        "\n",
        "    def _step(self, in_data, user_data=None, debug=None):\n",
        "        init_debug(debug, {\n",
        "            \"read_head\": {},\n",
        "            \"write_head\": {},\n",
        "            \"temporal_links\": {}\n",
        "        })\n",
        "\n",
        "        # input shape: [ batch, channels ]\n",
        "        batch_size = in_data.size(0)\n",
        "\n",
        "        # run the controller\n",
        "        prev_read_data = self.read_head.get_prev_data(self.memory).view([batch_size, -1])\n",
        "        if isinstance(user_data, torch.Tensor):\n",
        "            control_data = self.controller(torch.cat([in_data, prev_read_data], -1), torch.cat([user_data, prev_read_data], -1))\n",
        "        else:\n",
        "            control_data = self.controller(torch.cat([in_data, prev_read_data], -1))\n",
        "\n",
        "        \n",
        "        # memory ops NOTE: this layer also takes care of the fact that the controller output is now the same shape as the word length.\n",
        "        controls = self.controller_to_controls(control_data).contiguous()\n",
        "        controls = controls.clamp(-self.clip_controller, self.clip_controller) if self.clip_controller is not None else controls\n",
        "        \n",
        "        shapes = [[self.write_head.get_neural_input_size()], [self.read_head.get_neural_input_size()]]\n",
        "        \n",
        "        if self.sharpness_control is not None:\n",
        "            shapes.append(self.sharpness_control.get_neural_input_size())\n",
        "\n",
        "        tensors = split_tensor(controls, shapes)\n",
        "\n",
        "        write_head_control, read_head_control = tensors[:2]\n",
        "        tensors = tensors[2:]\n",
        "\n",
        "        prev_read_dist = self.read_head.get_prev_dist(self.memory)\n",
        "\n",
        "        self.memory = self.write_head(self.memory, write_head_control, prev_read_dist, debug=dict_get(debug,\"write_head\"))\n",
        "\n",
        "        prev_write = self.write_head.get_prev_write()\n",
        "        forward_dist, backward_dist = self.temporal_link(prev_write[\"write_dist\"] if prev_write is not None else None, prev_read_dist, debug=dict_get(debug, \"temporal_links\"))\n",
        "\n",
        "        if self.sharpness_control is not None:\n",
        "            forward_dist, backward_dist = self.sharpness_control(tensors[0], forward_dist, backward_dist)\n",
        "\n",
        "        read_data = self.read_head(self.memory, read_head_control, forward_dist, backward_dist, debug=dict_get(debug,\"read_head\"))\n",
        "\n",
        "        return self.controller_to_out(control_data) + self.read_to_out(read_data.view(batch_size,-1))\n",
        "\n",
        "    def _mem_init(self, batch_size, device):\n",
        "        if self.zero_mem_tensor is None or self.zero_mem_tensor.size(0)!=batch_size:\n",
        "            self.zero_mem_tensor = torch.zeros(batch_size, self.cell_count, self.word_length).to(device)\n",
        "\n",
        "        self.memory = self.zero_mem_tensor\n",
        "\n",
        "    def forward(self, in_data, user_data=None, debug=None):\n",
        "        self.write_head.new_sequence()\n",
        "        self.read_head.new_sequence()\n",
        "        self.temporal_link.new_sequence()\n",
        "        self.controller.new_sequence()\n",
        "\n",
        "        self._mem_init(in_data.size(0 if self.batch_first else 1), in_data.device)\n",
        "        \n",
        "        for t in range(in_data.size(1)):\n",
        "            if isinstance(user_data, torch.Tensor):\n",
        "                out_tsteps =  self._step(in_data[:,t], user_data[:,t])\n",
        "            else:\n",
        "                out_tsteps =  self._step(in_data[:,t]) \n",
        "        out = self.last_layer(out_tsteps)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C25jDbmYM5ay",
        "colab_type": "text"
      },
      "source": [
        "## LSTM/GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMQ0XMpKNSNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRU_LSTM_Model(torch.nn.Module):\n",
        "    def init_layer(self):\n",
        "        torch.nn.init.uniform_(self.fc.weight.data, -0.1, 0.1)\n",
        "        torch.nn.init.uniform_(self.fc.bias.data, -0.1, 0.1)\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, controller, clip_controller):\n",
        "        super(GRU_LSTM_Model, self).__init__()\n",
        "         \n",
        "        # Number of hidden layers\n",
        "        self.controller = controller\n",
        "        controller.init(input_dim)\n",
        "        self.clip_controller = clip_controller\n",
        "        \n",
        "        self.fc = torch.nn.Linear(controller.get_output_size(), output_dim)\n",
        "        self.init_layer()\n",
        "    \n",
        "    def forward(self, item_data, user_data=None):\n",
        "        self.controller.new_sequence()\n",
        "        \n",
        "        for t in range(item_data.size(1)):\n",
        "            if isinstance(user_data, torch.Tensor):\n",
        "                out_tsteps = self.controller(item_data[:,t], user_data[:,t])\n",
        "            else:\n",
        "                out_tsteps = self.controller(item_data[:,t])\n",
        "            out_tsteps.clamp(-self.clip_controller, self.clip_controller)\n",
        "        \n",
        "        out = self.fc(out_tsteps) \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJPwGil_AcHF",
        "colab_type": "text"
      },
      "source": [
        "## Controllers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaAormfBAZX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRUController(torch.nn.Module):\n",
        "    '''\n",
        "    Based on formula https://pytorch.org/docs/stable/nn.html\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, layer_sizes, out_from_all_layers=False, grad_clip=5.0):\n",
        "        super(GRUController, self).__init__()\n",
        "        self.out_from_all_layers = out_from_all_layers\n",
        "        self.grad_clip = grad_clip\n",
        "        self.hidden_size = layer_sizes\n",
        "        self.outputs = None\n",
        "    \n",
        "    def new_sequence(self):\n",
        "        self.outputs = None\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        def init_layer(l):\n",
        "            torch.nn.init.uniform_(l.weight.data, -0.1, 0.1)\n",
        "\n",
        "        init_layer(self.layer_item)\n",
        "        init_layer(self.layer_hidden)\n",
        "    \n",
        "    def _add_modules(self, name, m):\n",
        "        self.add_module(\"%s_%d\" % (name, 0), m)\n",
        "    \n",
        "    def init(self, input_size):\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.layer_item = torch.nn.Linear(self.input_size, 3 * self.hidden_size, bias=True)\n",
        "        self.layer_hidden = torch.nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=True)\n",
        "        \n",
        "        self.drop_item = torch.nn.Dropout(0.2)\n",
        "        \n",
        "        self._add_modules(\"layer_in\", self.layer_item)\n",
        "        self._add_modules(\"layer_hidden\", self.layer_hidden)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def get_output_size(self):\n",
        "        return self.hidden_size\n",
        "\n",
        "    def forward(self, input_item):        \n",
        "        if self.outputs is not None:\n",
        "            h = self.outputs\n",
        "        else:\n",
        "            h = torch.cuda.FloatTensor(input_item.size(0), self.hidden_size).fill_(0)\n",
        "                \n",
        "        gi = self.drop_item(self.layer_item(input_item))        \n",
        "        gh = self.layer_hidden(h)            \n",
        "        \n",
        "        i_r, i_i, i_n = gi.chunk(3, 1)\n",
        "        h_r, h_i, h_n = gh.chunk(3, 1)\n",
        "        \n",
        "        r = torch.sigmoid(i_r + h_r)\n",
        "        z = torch.sigmoid(i_i + h_i)\n",
        "\n",
        "        n = torch.tanh(i_n + r * h_n)\n",
        "        self.outputs = (1 - z) * n + z * h\n",
        "\n",
        "        return self.outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPA8wdogHqsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_grad(v, min, max):\n",
        "    v_tmp = v.expand_as(v)\n",
        "    v_tmp.register_hook(lambda g: g.clamp(min, max))\n",
        "    return v_tmp\n",
        "\n",
        "class UserBasedGRUController(torch.nn.Module):\n",
        "    '''\n",
        "    Based on formula https://pytorch.org/docs/stable/nn.html and work of Donkers et al.,\n",
        "    https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/p152-donkers.pdf\n",
        "    '''\n",
        "    def __init__(self, hidden_size, out_from_all_layers=False):\n",
        "        super(UserBasedGRUController, self).__init__()\n",
        "        self.out_from_all_layers = out_from_all_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.outputs = None\n",
        "    \n",
        "    def new_sequence(self):\n",
        "        self.outputs = None\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        def init_layer(l):\n",
        "            torch.nn.init.uniform_(l.weight.data, -0.1, 0.1)\n",
        "            torch.nn.init.uniform_(l.bias.data, -0.1, 0.1)\n",
        "            \n",
        "        init_layer(self.layer_attention_item)               \n",
        "        init_layer(self.layer_item)\n",
        "        init_layer(self.layer_user)\n",
        "        init_layer(self.layer_hidden)\n",
        "    \n",
        "    def _add_modules(self, name, m):\n",
        "        self.add_module(\"%s_%d\" % (name, 0), m)\n",
        "    \n",
        "    def init(self, input_size):\n",
        "        self.input_size = input_size\n",
        "        \n",
        "        self.layer_attention_item = torch.nn.Linear(self.input_size*2 + self.hidden_size, self.input_size, bias=True)                \n",
        "        self.layer_item = torch.nn.Linear(self.input_size, 3 * self.hidden_size, bias=True)\n",
        "        self.layer_user = torch.nn.Linear(self.input_size, self.hidden_size, bias=True)\n",
        "        self.layer_hidden = torch.nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=True) \n",
        "        \n",
        "        self.drop_item = torch.nn.Dropout(0.2)\n",
        "        self.drop_user = torch.nn.Dropout(0.5)  \n",
        "        \n",
        "        self._add_modules(\"layer_attention_item\", self.layer_attention_item)        \n",
        "        self._add_modules(\"layer_item\", self.layer_item)\n",
        "        self._add_modules(\"layer_user\", self.layer_user)\n",
        "        self._add_modules(\"layer_hidden\", self.layer_hidden)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def get_output_size(self):\n",
        "        return self.hidden_size\n",
        "\n",
        "    def forward(self, input_item, input_user):\n",
        "        if self.outputs is not None:\n",
        "            h = self.outputs\n",
        "        else:\n",
        "            h = torch.cuda.FloatTensor(input_item.size(0), self.hidden_size).fill_(0)\n",
        "            \n",
        "        i_a = self.layer_attention_item(torch.cat((h, input_item, input_user), 1))\n",
        "        \n",
        "        a = torch.sigmoid(i_a)\n",
        "        gh = self.layer_hidden(h)\n",
        "        gi = self.drop_item(self.layer_item((1 - a)*input_item))\n",
        "        u_n = self.drop_user(self.layer_user(a*input_user))\n",
        "        \n",
        "        i_r, i_i, i_n = gi.chunk(3, 1)\n",
        "        h_r, h_i, h_n = gh.chunk(3, 1)\n",
        "        \n",
        "        r = torch.sigmoid(i_r + h_r)\n",
        "        u = torch.sigmoid(i_i + h_i)\n",
        "\n",
        "        k = torch.tanh((r*h_n) + i_n + u_n)\n",
        "        self.outputs = (1 - u)*h + u*k\n",
        "        return self.outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBc4iG9p-sTH",
        "colab_type": "text"
      },
      "source": [
        "## Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjgyzExeLJi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stops the training if validation loss doesn't improve after a given patience.\n",
        "    CREDITS GO TO: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=7, verbose=True, save_model=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "                            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        \n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "        \n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            #self.best_model = copy.deepcopy(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhoPcxWiELlY",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7nc6IprDvId",
        "colab_type": "text"
      },
      "source": [
        "## Load raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay3fLY2DDurz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_raw_dataset(subset_perc=0.1, head=False):\n",
        "    df = pd.read_csv(LASTFM_STORAGE_FOLDER, sep='\\t', error_bad_lines=False, quoting=csv.QUOTE_NONE, header=None)\n",
        "    # Preprocessing (timestamp is converted to UNIX as this makes ordering easier.)\n",
        "    df.columns = ['userId', 'timestamp', 'artist_mbid', 'artist_name', 'song_mbid', 'song_title']\n",
        "    df['userId'] = df.groupby(['userId']).ngroup()\n",
        "    df['artistId'] = df.groupby(['artist_name']).ngroup()\n",
        "    df['songId'] = df.groupby(['song_title']).ngroup()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp']).astype(np.int64) // 10**9\n",
        "    df = df.drop(columns=['artist_mbid', 'artist_name', 'song_title', 'song_mbid'])\n",
        "\n",
        "    df = df.sort_values(by=['timestamp'])\n",
        "    subset_len = int(len(df)*subset_perc)\n",
        "    \n",
        "    if head:\n",
        "        df = df.head(subset_len)\n",
        "    else:\n",
        "        df = df.tail(subset_len)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK7514kUD9CD",
        "colab_type": "text"
      },
      "source": [
        "## Load dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfX5qYkYEAI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(location_files, merge_train_val=False, exp_var_seq=False):\n",
        "    sequences = np.load(location_files, allow_pickle=True)\n",
        "    sequences_train, sequences_val, sequences_test = sequences[0], sequences[1], sequences[2]\n",
        "    \n",
        "    if merge_train_val:\n",
        "        merge = np.vstack((sequences_train, sequences_val))\n",
        "    else:\n",
        "        merge = sequences_train\n",
        "    \n",
        "    unique, counts = np.unique(merge, return_counts=True)\n",
        "    class_weights = dict(zip(unique, counts))\n",
        "    \n",
        "    # Sorts low to high.\n",
        "    sorted_list = sorted(class_weights, key=class_weights.__getitem__)\n",
        "    pos_not_popular = int((1/3)*len(sorted_list))\n",
        "    pos_popular = int((2/3)*len(sorted_list))\n",
        "    \n",
        "    # Items for calculation.\n",
        "    items_not_popular = sorted_list[:pos_not_popular]\n",
        "    items_semi_popular = sorted_list[pos_not_popular:pos_popular]\n",
        "    items_popular = sorted_list[pos_popular:]\n",
        "                               \n",
        "    print('N of items not popular: {}, N items semi-popular: {}, N items popular: {}'.format(len(items_not_popular), len(items_semi_popular), len(items_popular)))\n",
        "    \n",
        "    if (len(set(items_not_popular) & set(items_semi_popular)) > 0) | (len(set(items_semi_popular) & set(items_popular)) > 0) | (len(set(items_not_popular) & set(items_popular)) > 0):\n",
        "        print('THERE IS OVERLAP BETWEEN THE SETS!!')\n",
        "                               \n",
        "    train_X = sequences_train[:, :-1]\n",
        "    val_X = sequences_val[:, :-1]\n",
        "    test_X = sequences_test[:, :-1]\n",
        "\n",
        "    train_y = sequences_train[:, -1]\n",
        "    val_y = sequences_val[:, -1]\n",
        "    test_y = sequences_test[:, -1]\n",
        "    \n",
        "    size_voca = sequences_train[:,:,1].max() + 1\n",
        "    size_users = sequences_train[:,:,0].max() + 1\n",
        "    \n",
        "    sequences_train, sequences_val, sequences_test = None, None, None\n",
        "    \n",
        "    tensor_1 = torch.from_numpy(train_X).long()\n",
        "    tensor_2 = torch.from_numpy(train_y).long()        \n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(tensor_1, tensor_2)\n",
        "\n",
        "    tensor_1 = torch.from_numpy(test_X).long()\n",
        "    tensor_2 = torch.from_numpy(test_y).long()\n",
        "\n",
        "    test = torch.utils.data.TensorDataset(tensor_1, tensor_2)\n",
        "\n",
        "    tensor_1 = torch.from_numpy(val_X).long()\n",
        "    tensor_2 = torch.from_numpy(val_y).long()\n",
        "\n",
        "    val = torch.utils.data.TensorDataset(tensor_1, tensor_2)\n",
        "\n",
        "    # Embedding size and the out size equals the amount of items in the vocabulary\n",
        "    in_size = embedding_size\n",
        "\n",
        "    if merge_train_val:\n",
        "        dataset = torch.utils.data.ConcatDataset((dataset, val))\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=4, pin_memory=True, drop_last=True, shuffle=True)\n",
        "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=4, pin_memory=True, drop_last=True, shuffle=False)\n",
        "        return train_loader, test_loader, size_voca, size_users, items_not_popular, items_semi_popular, items_popular\n",
        "    else:  \n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=4, pin_memory=True, drop_last=True, shuffle=True)\n",
        "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=4, pin_memory=True, drop_last=True, shuffle=False)\n",
        "        val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, num_workers=4, pin_memory=True, drop_last=True, shuffle=False)\n",
        "        return train_loader, val_loader, test_loader, size_voca, size_users"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kllf_4yEQDs",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sRA_zEPEPaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "#\n",
        "# Copyright 2017 Robert Csordas. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "#!export CUDA_VISIBLE_DEVICES=1\n",
        "def train(n_epochs, size_voca, train_loader, test_loader, val_loader=None, size_users=None, items_not_popular=None, items_semi_popular=None, items_popular=None):\n",
        "    def run_model(model_input):\n",
        "        if size_users:\n",
        "            user_input = embedding_user(model_input[:,:,0])\n",
        "            item_input = embedding_item(model_input[:,:,1])\n",
        "            \n",
        "            return model(item_input, user_input)\n",
        "        else:\n",
        "            item_input = embedding_item(model_input[:,:,1])\n",
        "            return model(item_input)\n",
        "    \n",
        "    def multiply_grads(params, mul):\n",
        "        if mul==1:\n",
        "            return\n",
        "\n",
        "        for pa in params:\n",
        "            for p in pa[\"params\"]:\n",
        "                p.grad.data *= mul\n",
        "    \n",
        "    def evaluate_set(data_loader, loss_only=True):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            losses = []\n",
        "            scores = {'recall20': [],\n",
        "                     'recall1': [],\n",
        "                     'mrr20': [],\n",
        "                     'recall20_p': [],\n",
        "                     'recall20_sp': [],\n",
        "                     'recall20_np': [],\n",
        "                     'recall1_p': [],\n",
        "                     'recall1_sp': [],\n",
        "                     'recall1_np': []}\n",
        "            for idx, (batch, labels) in enumerate(data_loader):\n",
        "                labels = labels.to(device)\n",
        "                batch = batch.to(device)\n",
        "                outputs = run_model(batch)\n",
        "                l = loss_func(outputs, labels[:,1])\n",
        "                losses.append(l.item())\n",
        "\n",
        "                if not loss_only:\n",
        "                    labels = labels[:,1].cpu().detach().numpy()\n",
        "                    _, output_indices = torch.topk(outputs, recall_k)\n",
        "                    output_indices = output_indices.cpu().detach().numpy()\n",
        "                    for i, top_k in enumerate(output_indices):\n",
        "                        if labels[i] == top_k[0]:\n",
        "                            scores['recall1'].append(1)\n",
        "                        else:\n",
        "                            scores['recall1'].append(0)\n",
        "                        \n",
        "                        if labels[i] in top_k:\n",
        "                            scores['recall20'].append(1)\n",
        "                            m = np.argwhere(np.array(top_k) == labels[i])[0][0] + 1\n",
        "                            scores['mrr20'].append(1/m)\n",
        "                        else:\n",
        "                            scores['mrr20'].append(0)\n",
        "                            scores['recall20'].append(0)\n",
        "                        \n",
        "                        if labels[i] in items_not_popular:\n",
        "                            if labels[i] == top_k[0]:\n",
        "                                scores['recall1_np'].append(1)\n",
        "                            else:\n",
        "                                scores['recall1_np'].append(0)\n",
        "                            \n",
        "                            if labels[i] in top_k:\n",
        "                                scores['recall20_np'].append(1)\n",
        "                            else:\n",
        "                                scores['recall20_np'].append(0)\n",
        "                        elif labels[i] in items_popular:\n",
        "                            if labels[i] == top_k[0]:\n",
        "                                scores['recall1_p'].append(1)\n",
        "                            else:\n",
        "                                scores['recall1_p'].append(0)\n",
        "                            \n",
        "                            if labels[i] in top_k:\n",
        "                                scores['recall20_p'].append(1)\n",
        "                            else:\n",
        "                                scores['recall20_p'].append(0)\n",
        "                        else:\n",
        "                            if labels[i] == top_k[0]:\n",
        "                                scores['recall1_sp'].append(1)\n",
        "                            else:\n",
        "                                scores['recall1_sp'].append(0)\n",
        "                            \n",
        "                            if labels[i] in top_k:\n",
        "                                scores['recall20_sp'].append(1)\n",
        "                            else:\n",
        "                                scores['recall20_sp'].append(0)         \n",
        "                        \n",
        "        model.train()\n",
        "        if not loss_only:\n",
        "            # Return Recall@1, Recall@20, MRR@20 and the loss.\n",
        "            return [np.mean(x) for x in scores.values()] + [np.mean(losses)]\n",
        "        else:\n",
        "            return np.mean(losses)\n",
        "    \n",
        "    \n",
        "    def load_checkpoint(model, optimizer, filename, embedding_item, embedding_user=None):\n",
        "        # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
        "        start_epoch = 0\n",
        "        checkpoint = torch.load(filename)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(filename, checkpoint['epoch']))\n",
        "        best_score = checkpoint['best_score']\n",
        "        embedding_item.load_state_dict(checkpoint['embedding_item'])\n",
        "        \n",
        "        if embedding_user:\n",
        "            embedding_user.load_state_dict(checkpoint['embedding_user'])\n",
        "        \n",
        "        # now individually transfer the optimizer parts...\n",
        "        for state in optimizer.state.values():\n",
        "            for k, v in state.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    state[k] = v.to(device)\n",
        "        \n",
        "        return model, optimizer, start_epoch, best_score, embedding_item, embedding_user\n",
        "    '''\n",
        "    Setup & train model.\n",
        "    '''\n",
        "        # If model exists.\n",
        "\n",
        "    if controller_type.lower() == 'gru':\n",
        "        print('Using GRU as the controller.')\n",
        "        controller = GRUController(hidden_size)\n",
        "    else:\n",
        "        controller = UserBasedGRUController(hidden_size)\n",
        "        print('Using a user-based GRU as the controller')\n",
        "        #controller_constructor = functools.partial(LSTMController, out_from_all_layers=lstm_use_all_outputs)\n",
        "    \n",
        "    # If model exists, use it.\n",
        "    if model_type.lower() == 'gru':\n",
        "        model = GRU_LSTM_Model(embedding_size, size_voca, controller, clip_controller)\n",
        "    else:\n",
        "        model = DNC(embedding_size, embedding_size, data_word_size, mem_count, n_read_heads, controller, \n",
        "                     batch_first=True, mask=masked_lookup, dealloc_content=dealloc_content, link_sharpness_control=sharpness_control,\n",
        "                     mask_min=mask_min, clip_controller=clip_controller, output_classes=size_voca, \n",
        "                     return_sequences=return_sequences)\n",
        "    \n",
        "    device = torch.device('cuda:0')\n",
        "    embedding_item = torch.nn.Embedding(size_voca, embedding_size).to(device)\n",
        "        \n",
        "    \n",
        "    early_stopping = EarlyStopping(patience=patience_early_stopping, \n",
        "                                  verbose=True)\n",
        "    \n",
        "    params = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not n.endswith(\".bias\") and (n != 'controller.layer_user.weight')], 'weight_decay': wd},\n",
        "        {'params': [p for n, p in model.named_parameters() if n.endswith(\".bias\")], 'weight_decay': 0}\n",
        "    ]\n",
        "    \n",
        "    torch.nn.init.uniform_(embedding_item.weight.data, -0.1, 0.1)\n",
        "    params.append({'params': embedding_item.parameters(), 'weight_decay': 0})\n",
        "\n",
        "    if size_users:\n",
        "        # Add l2-norm to user-specific layer.\n",
        "        params.append({'params': controller.layer_user.weight, 'weight_decay': wd_user})\n",
        "        embedding_user = torch.nn.Embedding(size_users, embedding_size).to(device)\n",
        "        torch.nn.init.uniform_(embedding_user.weight.data, -0.1, 0.1)\n",
        "        params.append({'params': embedding_user.parameters(), 'weight_decay': 0})\n",
        "    else:\n",
        "        embedding_user = None\n",
        "    \n",
        "    n_params = sum([sum([t.numel() for t in d['params']]) for d in params])\n",
        "    print('Number of params: {}'.format(n_params))            \n",
        "\n",
        "    optimizer = torch.optim.Adam(params, lr=lr)#, weight_decay=wd)\n",
        "    \n",
        "    if model_file is None:\n",
        "        start_epoch = 0\n",
        "    else:\n",
        "        if os.path.isfile(model_file):\n",
        "            print('Model loaded.')\n",
        "            model, optimizer, start_epoch, early_stop_bs, embedding_item, embedding_user = load_checkpoint(model, optimizer, model_file, embedding_item, embedding_user)\n",
        "            if early_stop_bs:\n",
        "                early_stopping.best_score = early_stop_bs\n",
        "                early_stopping.val_loss_min = early_stop_bs*(-1)\n",
        "        else:\n",
        "            start_epoch = 0\n",
        "     \n",
        "    # Model and layers to device.\n",
        "    loss_func = torch.nn.CrossEntropyLoss()\n",
        "    model = model.to(device)\n",
        "\n",
        "    embedding_item = embedding_item.to(device)\n",
        "    if size_users:\n",
        "        embedding_user = embedding_user.to(device)\n",
        "    \n",
        "    # Train model.  \n",
        "    i = 0\n",
        "    loss_sum = 0\n",
        "    iter_start_time = time.time()\n",
        "    data_load_total_time = 0\n",
        "    hyperparams = {}\n",
        "    \n",
        "    # Sanity.\n",
        "    model.train()\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        print('Starting new epoch.')\n",
        "        start = time.time()\n",
        "        data_load_timer = time.time()\n",
        "        for data, labels in train_loader:\n",
        "            labels = labels.to(device)\n",
        "            data = data.to(device)\n",
        "            data_load_total_time += time.time() - data_load_timer\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            n_subbatch = math.ceil(data.numel() / max_input_count_per_batch)\n",
        "            real_batch = max(math.floor(batch_size/n_subbatch),1)\n",
        "            n_subbatch = math.ceil(batch_size/real_batch)\n",
        "            remaining_batch = batch_size % real_batch\n",
        "            start_batch = time.time()\n",
        "\n",
        "            for subbatch in range(n_subbatch):\n",
        "                input = data\n",
        "                target = labels\n",
        "\n",
        "                if n_subbatch!=1:\n",
        "                    input = input[subbatch * real_batch: (subbatch + 1) * real_batch]\n",
        "                    target = target[subbatch * real_batch:(subbatch + 1) * real_batch]\n",
        "\n",
        "                f2 = data.clone()\n",
        "                f2 = input\n",
        "                output = run_model(f2)\n",
        "                \n",
        "                l = loss_func(output, target[:,1])\n",
        "                l.backward()\n",
        "\n",
        "                if remaining_batch!=0 and subbatch == n_subbatch-2:\n",
        "                    multiply_grads(params, real_batch/remaining_batch)\n",
        "\n",
        "            if n_subbatch!=1:\n",
        "                if remaining_batch==0:\n",
        "                    multiply_grads(params, 1/n_subbatch)\n",
        "                else:\n",
        "                     (params, remaining_batch / batch_size)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            i += 1\n",
        "            curr_loss = l.data.item()\n",
        "            loss_sum += curr_loss\n",
        "            if i % int(len(train_loader) / 15) == 0:\n",
        "                tim = time.time()\n",
        "                loss_avg = loss_sum / i\n",
        "\n",
        "                message = \"Iteration %d, loss: %.4f\" % (i, loss_avg)\n",
        "                message += \" (%.2f ms/iter, load time %.2g ms/iter)\" % (\n",
        "                            (tim - iter_start_time) / i * 1000.0,\n",
        "                            data_load_total_time / i * 1000.0)\n",
        "                print(message)\n",
        "\n",
        "                iter_start_time = tim\n",
        "                loss_sum = 0\n",
        "                data_load_total_time = 0\n",
        "            data_load_timer = time.time()\n",
        "            \n",
        "        # End of epoch, save model.\n",
        "        if model_file is not None:\n",
        "            state = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
        "                 'optimizer': optimizer.state_dict(), 'best_score': early_stopping.best_score,\n",
        "                    'embedding_item': embedding_item.state_dict()}\n",
        "            if size_users:\n",
        "                state['embedding_user'] = embedding_user.state_dict()\n",
        "            torch.save(state, model_file)\n",
        "\n",
        "        # End of epoch evaluation.\n",
        "        if val_loader:\n",
        "            val_loss = evaluate_set(val_loader)\n",
        "            \n",
        "            print('End of Epoch: {}, val loss {}'.format(time.time() - start, val_loss))\n",
        "            #scheduler.step(val_loss)\n",
        "            early_stopping(val_loss, model)\n",
        "            if early_stopping.early_stop:\n",
        "                print('Early stoppage triggered.')\n",
        "\n",
        "                #results_val = evaluate_set(val_loader, True, loss_only=False)\n",
        "                print('Training ended after {} and {} epochs, best validation score: {}'.format(time.time() - iter_start_time, epoch+1, early_stopping.best_score*(-1)))\n",
        "                print('-----------------------------------------------------------------')\n",
        "                hyperparams['n_epochs'] = np.max([epoch + 1 - early_stopping.patience, 1])\n",
        "                hyperparams['val_loss'] = early_stopping.best_score * (-1)\n",
        "                # Return parameters.\n",
        "                return hyperparams\n",
        "        else:\n",
        "            print('End of Epoch: {}'.format(time.time() - start))\n",
        "    \n",
        "    if not val_loader:\n",
        "        results_test = evaluate_set(test_loader, loss_only=False)\n",
        "        print('End of training, test results {}'.format(results_test))\n",
        "        return results_test\n",
        "    else:\n",
        "        print('Amount of epochs exceeded, but training has not finished! RESTART.')\n",
        "        return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA-v01srJdKV",
        "colab_type": "text"
      },
      "source": [
        "## Exp 1: Fold over entire dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0EYmU-T646u",
        "colab_type": "text"
      },
      "source": [
        "This section contains the code for the first experiment regarding the sample that was taken for the previous work performed by Donkers et al. The experiment finds whether or not the threshold for removal results in a decrease in Recall@K scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vmvh5-9DoId",
        "colab_type": "text"
      },
      "source": [
        "### Generate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUhpLLEnDrX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_sequences_exp_1(df, threshold_removal=20):    \n",
        "    for idx_df, df_sub in enumerate(np.array_split(df, 10)):\n",
        "        cnts = np.unique(df_sub['songId'], return_counts=True)\n",
        "        items_to_replace = [cnts[0][i] for i, v in enumerate(cnts[1]) if v <= threshold_removal]\n",
        "        df_sub = df_sub[~df_sub['songId'].isin(items_to_replace)]\n",
        "\n",
        "        df_sub = df_sub.sort_values(by=['songId'])\n",
        "        df_sub['songId'] = df_sub.groupby(['songId']).ngroup()\n",
        "        df_sub = df_sub.sort_values(by=['timestamp'])\n",
        "\n",
        "        sequences_train_usr, sequences_val_usr, sequences_test_usr = {}, {}, {}\n",
        "        unique_items_train = []\n",
        "        for usr in df_sub['userId'].unique():\n",
        "            df_usr = df_sub[df_sub['userId'] == usr].sort_values(by=['timestamp'])\n",
        "            seq_usr = df_usr[['userId', 'songId']].values\n",
        "\n",
        "            seq_train_len = int(len(seq_usr) * 0.9)\n",
        "            seq_val_len = seq_train_len + int((len(seq_usr) - seq_train_len)/2)\n",
        "            seq_train, seq_val, seq_test = seq_usr[:seq_train_len], seq_usr[seq_train_len:seq_val_len], seq_usr[seq_val_len:]\n",
        "\n",
        "            sequences_train_usr[usr] = seq_train\n",
        "            sequences_val_usr[usr] = seq_val\n",
        "            sequences_test_usr[usr] = seq_test\n",
        "\n",
        "            unique_items_train.extend(seq_train[:,1])\n",
        "\n",
        "        unique_items_train = list(set(unique_items_train))\n",
        "        sequences_test, sequences_val, sequences_train = [], [], []\n",
        "        for i, usr in enumerate(sequences_test_usr.keys()):\n",
        "            seq_test = sequences_test_usr[usr]\n",
        "            seq_train = sequences_train_usr[usr]\n",
        "            seq_val = sequences_val_usr[usr]\n",
        "\n",
        "            seq_val = np.delete(seq_val, np.where(~np.isin(seq_val[:,1], unique_items_train)), axis=0)\n",
        "            seq_test = np.delete(seq_test, np.where(~np.isin(seq_test[:,1], unique_items_train)), axis=0)\n",
        "\n",
        "            if len(seq_test) >= MAX_LENGTH_SEQUENCE:\n",
        "                sequences_test.extend([seq_test[n:n+MAX_LENGTH_SEQUENCE] for n in range(len(seq_test)-MAX_LENGTH_SEQUENCE+1)])\n",
        "            # Val set\n",
        "            if len(seq_val) >= MAX_LENGTH_SEQUENCE:\n",
        "                sequences_val.extend([seq_val[n:n+MAX_LENGTH_SEQUENCE] for n in range(len(seq_val)-MAX_LENGTH_SEQUENCE+1)])\n",
        "\n",
        "            # Train set\n",
        "            if len(seq_train) >= MAX_LENGTH_SEQUENCE:\n",
        "                sequences_train.extend([seq_train[n:n+MAX_LENGTH_SEQUENCE] for n in range(len(seq_train)-MAX_LENGTH_SEQUENCE+1)])\n",
        "\n",
        "        # Create and save arrays\n",
        "        sequences_test = np.array(sequences_test)\n",
        "        sequences_train = np.array(sequences_train)\n",
        "        sequences_val = np.array(sequences_val)\n",
        "        \n",
        "        if threshold_removal == 2:\n",
        "            np.save('{}/exp1/sequences_{}_2.npy'.format(HOME_FOLDER, idx_df), np.array([sequences_train, sequences_val, sequences_test]))\n",
        "        else:\n",
        "            np.save('{}/exp1/sequences_{}.npy'.format(HOME_FOLDER, idx_df), np.array([sequences_train, sequences_val, sequences_test]))\n",
        "        \n",
        "        print('Shape train sequences: {}, shape val sequences: {}, shape test sequences: {}'.format(sequences_train.shape, sequences_val.shape, sequences_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIBv-9_tFKX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = load_raw_dataset(subset_perc=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2QYJcLhCUs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for threshold_removal in [20, 2]:\n",
        "#     gen_sequences_exp_1(df, threshold_removal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddgY-6YeDp2F",
        "colab_type": "text"
      },
      "source": [
        "### Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7xEqOLCzH3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Variable settings\n",
        "batch_size = 1000\n",
        "lr = 0.001\n",
        "\n",
        "controller_types = ['gru', 'gru_user']\n",
        "wd = 0\n",
        "wd_user = 0.01\n",
        "name = 'random_train'\n",
        "gpu = 0\n",
        "clip_controller = 5.0\n",
        "recall_k = 20\n",
        "\n",
        "patience_early_stopping = 1\n",
        "embedding_size = 1000\n",
        "hidden_size = 1000\n",
        "max_input_count_per_batch = MAX_LENGTH_SEQUENCE * batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1vsc1587kBG",
        "colab_type": "text"
      },
      "source": [
        "####  1.1: Removal threshold equal to 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kryOIfqs9cOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 25\n",
        "n_folds = 10\n",
        "model_file = None\n",
        "\n",
        "for user_based in [0, 1]:\n",
        "    controller_type = controller_types[user_based]\n",
        "    file_results = '{}/exp1/results_user_based_{}.npy'.format(HOME_FOLDER, user_based)\n",
        "    if os.path.isfile(file_results):\n",
        "        results = list(np.load(file_results))\n",
        "        if len(results) == n_folds:\n",
        "            continue\n",
        "        print('Result file loaded {}, starting at {}.'.format(user_based, len(results)))\n",
        "    else:\n",
        "        results = []\n",
        "    \n",
        "    for idx_f in range(len(results), n_folds):\n",
        "        file_url = '{}/exp1/sequences_{}.npy'.format(HOME_FOLDER, idx_f)\n",
        "        print(file_url)\n",
        "        \n",
        "        train_loader, val_loader, test_loader, size_voca, size_users = load_data(location_files=file_url)\n",
        "        \n",
        "        if not bool(user_based):\n",
        "            size_users = None\n",
        "\n",
        "        hyperparams = train(n_epochs, size_voca, train_loader, test_loader, val_loader, size_users)\n",
        "\n",
        "        if hyperparams == -1:\n",
        "            break\n",
        "        \n",
        "        print('Following hyperparameters were found: {}'.format(hyperparams))\n",
        "\n",
        "        train_loader, test_loader, size_voca, size_users = load_data(location_files=file_url, merge_train_val=True)\n",
        "        if not bool(user_based):\n",
        "            size_users = None\n",
        "\n",
        "        fold_results = train(hyperparams['n_epochs'], size_voca, train_loader, test_loader, val_loader=None, size_users=size_users)\n",
        "        results.append([fold_results, hyperparams])\n",
        "        np.save(file_results, np.array(results))\n",
        "        print('Fold {}, Following results were found: {}'.format(idx_f, fold_results))\n",
        "        print('----------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr2_HoBC7pVT",
        "colab_type": "text"
      },
      "source": [
        "####  1.2: Removal threshold equal to 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01HrsKhrX1yN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 25\n",
        "n_folds = 10\n",
        "model_file = None\n",
        "\n",
        "for user_based in [0, 1]:\n",
        "    controller_type = controller_types[user_based]\n",
        "    file_results = '{}/exp1/results_user_based_{}_2.npy'.format(HOME_FOLDER, user_based)\n",
        "    if os.path.isfile(file_results):\n",
        "        results = list(np.load(file_results))\n",
        "        if len(results) == n_folds:\n",
        "            continue\n",
        "        print('Result file loaded {}, starting at {}.'.format(user_based, len(results)))\n",
        "    else:\n",
        "        results = []\n",
        "    \n",
        "    for idx_f in range(len(results), n_folds):\n",
        "        file_url = '{}/exp1/sequences_{}_2.npy'.format(HOME_FOLDER, idx_f)\n",
        "        print(file_url)\n",
        "        \n",
        "        train_loader, val_loader, test_loader, size_voca, size_users = load_data(location_files=file_url)\n",
        "        \n",
        "        if not bool(user_based):\n",
        "            size_users = None\n",
        "        print(size_voca, size_users)\n",
        "        hyperparams = train(n_epochs, size_voca, train_loader, test_loader, val_loader, size_users)\n",
        "\n",
        "        if hyperparams == -1:\n",
        "            break\n",
        "        \n",
        "        print('Following hyperparameters were found: {}'.format(hyperparams))\n",
        "\n",
        "        train_loader, test_loader, size_voca, size_users = load_data(location_files=file_url, merge_train_val=True)\n",
        "        if not bool(user_based):\n",
        "            size_users = None\n",
        "\n",
        "        fold_results = train(hyperparams['n_epochs'], size_voca, train_loader, test_loader, val_loader=None, size_users=size_users)\n",
        "        results.append([fold_results, hyperparams])\n",
        "        np.save(file_results, np.array(results))\n",
        "        print('Fold {}, Following results were found: {}'.format(idx_f, fold_results))\n",
        "        print('----------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAFd11kU8_vJ",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy2o1KSq94BN",
        "colab_type": "text"
      },
      "source": [
        "#### Recall@20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYU4AUbjCXE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = '{}/exp1/results_user_based_{}.npy'.format(HOME_FOLDER, usr_based)\n",
        "results_n_ub = [x[0][1] for x in np.load(file_path.format(0), allow_pickle=True)]\n",
        "results_ub = [x[0][1] for x in np.load(file_path.format(1), allow_pickle=True)]\n",
        "\n",
        "file_path = '{}/exp1/results_user_based_{}_2.npy'.format(HOME_FOLDER, usr_based)\n",
        "results_n_ub_2 = [x[0][1] for x in np.load(file_path.format(0), allow_pickle=True)]\n",
        "results_ub_2 = [x[0][1] for x in np.load(file_path.format(1), allow_pickle=True)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8q37wTTjTRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(columns=['Fold', 'GRU user-based r=20', 'Vanilla GRU r=20', 'GRU user-based r=2', 'Vanilla GRU r=2'])\n",
        "for fold in range(10):\n",
        "    temp_res = [int(fold)+1, np.round(results_ub[fold], 3), np.round(results_n_ub[fold], 3), np.round(results_ub_2[fold], 3), np.round(results_n_ub_2[fold], 3)]\n",
        "    df.loc[fold] = temp_res\n",
        "print(df.to_latex(index=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFyGyidwkO5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.subplot(111)\n",
        "ax.scatter(list(range(1, len(results_ub)+1)), results_ub, label='User-based GRU, r=20')\n",
        "ax.scatter(list(range(1, len(results_ub)+1)), results_ub_2, label='User-based GRU, r=2')\n",
        "ax.scatter(list(range(1, len(results_ub)+1)), results_n_ub, label='Vanilla GRU, r=20')\n",
        "ax.scatter(list(range(1, len(results_ub)+1)), results_n_ub_2, label='Vanilla GRU, r=2')\n",
        "plt.ylabel('Recall@20')\n",
        "plt.xlabel('Subsets')\n",
        "plt.ylim(0.2, 0.45)\n",
        "plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "ax.legend()\n",
        "plt.savefig('{}/exp1/recall@20_plot_exp1.png'.format(HOME_FOLDER))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r8gmo1j98DL",
        "colab_type": "text"
      },
      "source": [
        "#### Recall@1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RVgRuWOU06CW",
        "colab": {}
      },
      "source": [
        "file_path = '{}/exp1/results_user_based_{}.npy'.format(HOME_FOLDER, usr_based)\n",
        "results_n_ub = [x[0][2] for x in np.load(file_path.format(0), allow_pickle=True)]\n",
        "results_ub = [x[0][2] for x in np.load(file_path.format(1), allow_pickle=True)]\n",
        "\n",
        "file_path = '{}/exp1/results_user_based_{}_2.npy'.format(HOME_FOLDER, usr_based)\n",
        "results_n_ub_2 = [x[0][2] for x in np.load(file_path.format(0), allow_pickle=True)]\n",
        "results_ub_2 = [x[0][2] for x in np.load(file_path.format(1), allow_pickle=True)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkk_ydSTnGxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(columns=['Fold', 'GRU user-based r=20', 'Vanilla GRU r=20', 'GRU user-based r=2', 'Vanilla GRU r=2'])\n",
        "for fold in range(10):\n",
        "    temp_res = [int(fold)+1, np.round(results_ub[fold], 3), np.round(results_n_ub[fold], 3), np.round(results_ub_2[fold], 3), np.round(results_n_ub_2[fold], 3)]\n",
        "    df.loc[fold] = temp_res\n",
        "\n",
        "print(df.to_latex(index=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FHKhqy4l06Cf",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.subplot(111)\n",
        "ax.scatter(list(range(1, len(results_ub)+1)), results_ub, label='User-based GRU, r=20')\n",
        "ax.scatter(list(range(1, len(results_ub)+1)), results_ub_2, label='User-based GRU, r=2')\n",
        "ax.scatter(list(range(1, len(results_ub)+1)), results_n_ub, label='Vanilla GRU, r=20')\n",
        "ax.scatter(list(range(1, len(results_ub)+1)), results_n_ub_2, label='Vanilla GRU, r=2')\n",
        "plt.ylabel('Recall@1')\n",
        "plt.ylim(0.2, 0.3)\n",
        "plt.xlabel('Subsets')\n",
        "plt.xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "ax.legend()\n",
        "plt.savefig('/content/gdrive/My Drive/projects/thesis/sequences_gru_test/exp1/recall@1_plot_exp1.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r2DWiWYeP1I2"
      },
      "source": [
        "## Exp 2: Varying training data size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sRPOoeLkP1I6"
      },
      "source": [
        "### Generate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G5htPSDgP1I8",
        "colab": {}
      },
      "source": [
        "def gen_sequences_exp_2(df1, df2):\n",
        "    '''\n",
        "    NOTE: Preprocess frames of 10% each separately, then merge them. This makes for a fair comparison\n",
        "    with the other experiments.\n",
        "    '''\n",
        "    \n",
        "    cnts = np.unique(df1['songId'], return_counts=True)\n",
        "    items_to_replace = [cnts[0][i] for i, v in enumerate(cnts[1]) if v <= MIN_THRESH]\n",
        "    df1 = df1[~df1['songId'].isin(items_to_replace)]\n",
        "\n",
        "    cnts = np.unique(df2['songId'], return_counts=True)\n",
        "    items_to_replace = [cnts[0][i] for i, v in enumerate(cnts[1]) if v <= MIN_THRESH]\n",
        "    df2 = df2[~df2['songId'].isin(items_to_replace)]\n",
        "    \n",
        "    df = df1.append(df2)\n",
        "    \n",
        "    # Sort and re-assign songIds AFTER merging, else we would get missmatches.\n",
        "    df = df.sort_values(by=['songId'])\n",
        "    df['songId'] = df.groupby(['songId']).ngroup()\n",
        "    df = df.sort_values(by=['timestamp'])\n",
        "    \n",
        "    for idx_df, remove_perc in enumerate([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
        "        sequences_train_usr, sequences_val_usr, sequences_test_usr = {}, {}, {}\n",
        "        unique_items_train = []\n",
        "        for usr in df['userId'].unique():\n",
        "            df_usr = df[df['userId'] == usr].sort_values(by=['timestamp'])\n",
        "            seq_usr = df_usr[['userId', 'songId']].values\n",
        "            \n",
        "            seq_train_start = int(len(seq_usr) * remove_perc)\n",
        "            seq_val_len = int(len(seq_usr) * 0.95)\n",
        "            seq_test_len = seq_val_len + int((len(seq_usr) - seq_val_len)/2)\n",
        "            seq_train, seq_val, seq_test = seq_usr[seq_train_start:seq_val_len], seq_usr[seq_val_len:seq_test_len], seq_usr[seq_test_len:]\n",
        "\n",
        "            sequences_train_usr[usr] = seq_train\n",
        "            sequences_val_usr[usr] = seq_val\n",
        "            sequences_test_usr[usr] = seq_test\n",
        "\n",
        "            unique_items_train.extend(seq_train[:,1])\n",
        "\n",
        "        unique_items_train = list(set(unique_items_train))\n",
        "        sequences_test, sequences_val, sequences_train = [], [], []\n",
        "        for i, usr in enumerate(sequences_test_usr.keys()):\n",
        "            seq_test = sequences_test_usr[usr]\n",
        "            seq_train = sequences_train_usr[usr]\n",
        "            seq_val = sequences_val_usr[usr]\n",
        "\n",
        "            seq_val = np.delete(seq_val, np.where(~np.isin(seq_val[:,1], unique_items_train)), axis=0)\n",
        "            seq_test = np.delete(seq_test, np.where(~np.isin(seq_test[:,1], unique_items_train)), axis=0)\n",
        "\n",
        "            if len(seq_test) >= MAX_LENGTH_SEQUENCE:\n",
        "                sequences_test.extend([seq_test[n:n+MAX_LENGTH_SEQUENCE] for n in range(len(seq_test)-MAX_LENGTH_SEQUENCE+1)])\n",
        "            # Val set\n",
        "            if len(seq_val) >= MAX_LENGTH_SEQUENCE:\n",
        "                sequences_val.extend([seq_val[n:n+MAX_LENGTH_SEQUENCE] for n in range(len(seq_val)-MAX_LENGTH_SEQUENCE+1)])\n",
        "\n",
        "            # Train set\n",
        "            if len(seq_train) >= MAX_LENGTH_SEQUENCE:\n",
        "                sequences_train.extend([seq_train[n:n+MAX_LENGTH_SEQUENCE] for n in range(len(seq_train)-MAX_LENGTH_SEQUENCE+1)])\n",
        "\n",
        "        # Create and save arrays\n",
        "        sequences_test = np.array(sequences_test)\n",
        "        sequences_train = np.array(sequences_train)\n",
        "        sequences_val = np.array(sequences_val)  \n",
        "\n",
        "        np.save('{}/exp2/sequences_{}.npy'.format(HOME_FOLDER, idx_df), np.array([sequences_train, sequences_val, sequences_test]))\n",
        "        print('Shape train sequences: {}, shape val sequences: {}, shape test sequences: {}'.format(sequences_train.shape, sequences_val.shape, sequences_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tGIgucZrP1JG",
        "colab": {}
      },
      "source": [
        "df = load_raw_dataset(subset_perc=0.5, head=True)\n",
        "df = df.tail(int(len(df) * (2/5)))\n",
        "df1, df2 = np.array_split(df, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oveq3WkpP1JQ",
        "colab": {}
      },
      "source": [
        "gen_sequences_exp_2(df1, df2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ud7vmZ_P1JY"
      },
      "source": [
        "### Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gl9SzJnUP1Ja",
        "colab": {}
      },
      "source": [
        "# Variable settings\n",
        "batch_size = 1000\n",
        "lr = 0.001\n",
        "\n",
        "controller_types = ['gru', 'gru_user']\n",
        "wd = 0\n",
        "wd_user = 0.01\n",
        "name = 'random_train'\n",
        "gpu = 0\n",
        "clip_controller = 5.0\n",
        "recall_k = 20\n",
        "\n",
        "patience_early_stopping = 1\n",
        "embedding_size = 1000\n",
        "hidden_size = 1000\n",
        "\n",
        "# For sequence lengths:   \n",
        "max_input_count_per_batch = MAX_LENGTH_SEQUENCE * batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3BBQPZCWP1Je",
        "colab": {}
      },
      "source": [
        "n_epochs = 25\n",
        "n_folds = 10\n",
        "\n",
        "model_file = None\n",
        "\n",
        "for user_based in [0, 1]:\n",
        "    model_type = 'gru'\n",
        "    controller_type = controller_types[user_based]\n",
        "    file_results = '{}/exp2/results_user_based_{}.npy'.format(HOME_FOLDER, user_based)\n",
        "    if os.path.isfile(file_results):\n",
        "        results = list(np.load(file_results))\n",
        "        if len(results) == n_folds:\n",
        "            continue\n",
        "        print('Result file loaded {}, starting at {}.'.format(user_based, len(results)))\n",
        "    else:\n",
        "        results = []\n",
        "    \n",
        "    for idx_fold in range(len(results), n_folds):\n",
        "        file_url = '{}/exp2/sequences_{}.npy'.format(HOME_FOLDER, idx_fold)\n",
        "        train_loader, val_loader, test_loader, size_voca, size_users = load_data(location_files=file_url)\n",
        "        if not bool(user_based):\n",
        "            size_users = None\n",
        "\n",
        "        hyperparams = train(n_epochs, size_voca, train_loader, test_loader, val_loader, size_users)\n",
        "\n",
        "        if hyperparams == -1:\n",
        "            break\n",
        "        \n",
        "        print('Following hyperparameters were found: {}'.format(hyperparams))\n",
        "        \n",
        "        train_loader, test_loader, size_voca, size_users = load_data(location_files=file_url, merge_train_val=True)\n",
        "        if not bool(user_based):\n",
        "            size_users = None\n",
        "\n",
        "        fold_results = train(hyperparams['n_epochs'], size_voca, train_loader, test_loader, val_loader=None, size_users=size_users)\n",
        "        results.append([fold_results, hyperparams])\n",
        "        np.save(file_results, np.array(results))\n",
        "        print('Fold {}, Following results were found: {}'.format(idx_fold, fold_results))\n",
        "        print('----------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuksYQGo--y9",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5v8YzTq_Bwd",
        "colab_type": "text"
      },
      "source": [
        "#### Recall@20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r74WTc1XykYD",
        "colab": {}
      },
      "source": [
        "file_path = '{}/exp2/results_user_based_{}.npy'.format(HOME_FOLDER)\n",
        "results_n_ub = [x[0][1] for x in np.load(file_path.format(0), allow_pickle=True)]\n",
        "results_ub = [x[0][1] for x in np.load(file_path.format(1), allow_pickle=True)]\n",
        "data_sizes = [1851095, 1665046, 1478678, 1292372, 1106009, 919600, 733415, 547202, 360959, 174996]\n",
        "data_sizes = [int(x/1000) for x in data_sizes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e97RQtFDngE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(columns=['GRU user-based', 'Vanilla GRU', 'Size training set in thousands'])\n",
        "for idx, size in enumerate(data_sizes):\n",
        "    temp_res = [np.round(results_ub[idx], 3), np.round(results_n_ub[idx], 3), size]\n",
        "    df.loc[idx] = temp_res\n",
        "\n",
        "print(df.to_latex(index=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dviWsGdqykYH",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.subplot(111)\n",
        "ax.plot(data_sizes, results_ub, label='User-based GRU',marker='o')\n",
        "ax.plot(data_sizes, results_n_ub, label='Vanilla GRU',marker='o')\n",
        "plt.ylabel('Recall@20')\n",
        "plt.xlabel('Number of training sequences in thousands')\n",
        "ax.legend()\n",
        "plt.savefig('{}/exp2/recall@20_plot_exp2.png'.format(HOME_FOLDER))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quHne06m_RlU",
        "colab_type": "text"
      },
      "source": [
        "#### Recall@1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ab7P8wP3L95",
        "colab": {}
      },
      "source": [
        "file_path = '{}/exp2/results_user_based_{}.npy'.format(HOME_FOLDER)\n",
        "results_n_ub = [x[0][2] for x in np.load(file_path.format(0), allow_pickle=True)]\n",
        "results_ub = [x[0][2] for x in np.load(file_path.format(1), allow_pickle=True)]\n",
        "data_sizes = [1851095, 1665046, 1478678, 1292372, 1106009, 919600, 733415, 547202, 360959, 174996]\n",
        "data_sizes = [int(x/1000) for x in data_sizes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAVrWdWGoWlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(columns=['GRU user-based', 'Vanilla GRU', 'Size training set in thousands'])\n",
        "for idx, size in enumerate(data_sizes):\n",
        "    temp_res = [np.round(results_ub[idx], 3), np.round(results_n_ub[idx], 3), size]\n",
        "    df.loc[idx] = temp_res\n",
        "\n",
        "print(df.to_latex(index=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBnkyyaC3L-D",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.subplot(111)\n",
        "ax.plot(data_sizes, results_ub, label='Vanilla GRU',marker='o')\n",
        "ax.plot(data_sizes, results_n_ub, label='User-based GRU',marker='o')\n",
        "plt.ylabel('Recall@1')\n",
        "plt.xlabel('Number of training sequences in thousands')\n",
        "plt.xlim(np.max(data_sizes) + 50, np.min(data_sizes) - 50)\n",
        "ax.legend()\n",
        "plt.savefig('{}/exp2/recall@1_plot_exp2.png'.format(HOME_FOLDER))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1m7Q3mGhUysb"
      },
      "source": [
        "## Exp 3: Embedding size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DpxSnVwmUytI"
      },
      "source": [
        "### Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "US3VwJEtUytJ",
        "colab": {}
      },
      "source": [
        "# Variable settings\n",
        "batch_size = 1000\n",
        "lr = 0.001\n",
        "\n",
        "controller_types = ['gru', 'gru_user']\n",
        "wd = 0\n",
        "wd_user = 0.01\n",
        "name = 'random_train'\n",
        "gpu = 0\n",
        "clip_controller = 5.0\n",
        "recall_k = 20\n",
        "\n",
        "patience_early_stopping = 1\n",
        "hidden_size = 1000\n",
        "\n",
        "# For sequence lengths:   \n",
        "max_input_count_per_batch = MAX_LENGTH_SEQUENCE * batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AqXaAbyeUytO",
        "colab": {}
      },
      "source": [
        "n_epochs = 25\n",
        "\n",
        "embedding_sizes = [32, 64, 128, 256, 512, 1000, 2048, 4096, 8192, -1]\n",
        "file_set = '{}/exp1/sequences_{}.npy'\n",
        "model_type = 'gru'\n",
        "\n",
        "for idx_fold in [0, 1]:\n",
        "    for user_based in [0, 1]:\n",
        "        file_url = file_set.format(HOME_FOLDER, idx_fold)\n",
        "        controller_type = controller_types[user_based]\n",
        "        file_results = '{}/exp3/results_user_based_{}_f{}.npy'.format(HOME_FOLDER, user_based, idx_fold)\n",
        "        \n",
        "        if os.path.isfile(file_results):\n",
        "            results = list(np.load(file_results, allow_pickle=True))\n",
        "            if len(results) == len(embedding_sizes):\n",
        "                continue\n",
        "            print('Result file loaded {}, starting at {}.'.format(user_based, len(results)+1))\n",
        "        else:\n",
        "            results = []\n",
        "        for embedding_size in embedding_sizes[len(results):]:\n",
        "            model_file = '{}/exp3/models/model_type_{}_ub_{}_f{}_emb_{}_hp.pth'.format(HOME_FOLDER, model_type, user_based, idx_fold, embedding_size)\n",
        "\n",
        "            if embedding_size != -1:\n",
        "                train_loader, val_loader, test_loader, size_voca, size_users = load_data(location_files=file_url)\n",
        "                if not bool(user_based):\n",
        "                    size_users = None\n",
        "\n",
        "                hyperparams = train(n_epochs, size_voca, train_loader, test_loader, val_loader, size_users)\n",
        "\n",
        "                if hyperparams == -1:\n",
        "                    break\n",
        "\n",
        "                print('Following hyperparameters were found: {}'.format(hyperparams))\n",
        "\n",
        "                if not bool(user_based):\n",
        "                    size_users = None\n",
        "\n",
        "                hyperparams['embedding_size'] = embedding_size\n",
        "\n",
        "                results.append([hyperparams])\n",
        "                np.save(file_results, np.array(results))\n",
        "                print('Following results were found: {}'.format(results))\n",
        "                print('----------------------------------------')\n",
        "            elif not user_based:\n",
        "                train_loader, val_loader, test_loader, size_voca, size_users = load_data(location_files=file_url)\n",
        "                embedding_size = size_voca\n",
        "                if not bool(user_based):\n",
        "                    size_users = None\n",
        "\n",
        "                hyperparams = train(n_epochs, size_voca, train_loader, test_loader, val_loader, size_users)\n",
        "\n",
        "                if hyperparams == -1:\n",
        "                    break\n",
        "\n",
        "                print('Following hyperparameters were found: {}'.format(hyperparams))\n",
        "\n",
        "                if not bool(user_based):\n",
        "                    size_users = None\n",
        "\n",
        "                hyperparams['embedding_size'] = embedding_size\n",
        "                results.append([hyperparams])\n",
        "                np.save(file_results, np.array(results))\n",
        "                print('Following results were found: {}'.format(fold_results))\n",
        "                print('----------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ViGUUZVGIy2",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lgFGJegGvhln",
        "colab": {}
      },
      "source": [
        "file_path = '{}/exp3/results_user_based_{}_f{}.npy'.format(HOMEFOLDER)\n",
        "results = {}\n",
        "for ub in [0, 1]:\n",
        "    results[ub] = np.zeros((9))\n",
        "    for idx_fold in range(3):\n",
        "        temp = np.load(file_path.format(ub, idx_fold), allow_pickle=True)\n",
        "        temp = np.array([np.array(x[0]['val_loss']) for x in temp])\n",
        "        results[ub] += temp\n",
        "\n",
        "    results[ub] /= 3\n",
        "\n",
        "embed_sizes = [32, 64, 128, 256, 512, 1000, 2048, 4096, 8192]\n",
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d65NS2asQp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(columns=['GRU user-based', 'Vanilla GRU', 'Embedding size'])\n",
        "for idx, size in enumerate(embed_sizes):\n",
        "    temp_res = [np.round(results[1][idx], 3), np.round(results[0][idx], 3), size]\n",
        "    df.loc[idx] = temp_res\n",
        "print(df.to_latex(index=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eyqsPnL3vhlr",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = plt.subplot(111)\n",
        "ax.plot(embed_sizes, results[0], label='Vanilla GRU',marker='o')\n",
        "ax.plot(embed_sizes, results[1], label='User-based GRU',marker='o')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Embedding size')\n",
        "\n",
        "ticks = [32, 64, 128, 256, 512, 1000, 2048, 4096, 8192]\n",
        "xm = [1000, 2048, 4096, 8192]\n",
        "\n",
        "ax.set_xscale('log', basex=2)\n",
        "ax.set_xticks(embed_sizes)\n",
        "ax.set_xticks(xm, minor=True)\n",
        "ax.set_xticklabels(ticks)\n",
        "ax.set_xticklabels([\"\"]*len(xm), minor=True)\n",
        "\n",
        "plt.xticks(embed_sizes)\n",
        "ax.legend()\n",
        "plt.savefig('{}/exp3/loss_plot_exp3.png'.format(HOME_FOLDER))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdmn8AKGnbfx",
        "colab_type": "text"
      },
      "source": [
        "## Exp 4: Various sequence lengths DNC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xqf_fvdEHNC5"
      },
      "source": [
        "### Generate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fT_kiUlTHNC6",
        "colab": {}
      },
      "source": [
        "def gen_sequences_exp_4(df, visualize=False):    \n",
        "    for idx_df, df_sub in enumerate(np.array_split(df, 3)):\n",
        "        cnts = np.unique(df_sub['songId'], return_counts=True)\n",
        "        items_to_replace = [cnts[0][i] for i, v in enumerate(cnts[1]) if v <= MIN_THRESH]\n",
        "        df_sub = df_sub[~df_sub['songId'].isin(items_to_replace)]\n",
        "\n",
        "        df_sub = df_sub.sort_values(by=['songId'])\n",
        "        df_sub['songId'] = df_sub.groupby(['songId']).ngroup()\n",
        "        df_sub = df_sub.sort_values(by=['timestamp'])\n",
        "        \n",
        "        sequences_train_usr, sequences_val_usr, sequences_test_usr = {}, {}, {}\n",
        "        unique_items_train = []\n",
        "        \n",
        "        for usr in df_sub['userId'].unique():\n",
        "            df_usr = df_sub[df_sub['userId'] == usr].sort_values(by=['timestamp'])\n",
        "            seq_usr = df_usr[['userId', 'songId']].values\n",
        "\n",
        "            seq_train_len = int(len(seq_usr) * 0.9)\n",
        "            seq_val_len = seq_train_len + int((len(seq_usr) - seq_train_len)/2)\n",
        "            seq_train, seq_val, seq_test = seq_usr[:seq_train_len], seq_usr[seq_train_len:seq_val_len], seq_usr[seq_val_len:]\n",
        "\n",
        "            sequences_train_usr[usr] = seq_train\n",
        "            sequences_val_usr[usr] = seq_val\n",
        "            sequences_test_usr[usr] = seq_test\n",
        "\n",
        "            unique_items_train.extend(seq_train[:,1])\n",
        "        if visualize:\n",
        "            seq_visualize = np.array(list(sequences_train_usr.values())).flatten()\n",
        "            plt.hist(x=seq_visualize, bins=10)#, density=True)#histtype='step')\n",
        "            plt.xlabel('Item identification numbers')\n",
        "            plt.ylabel('Number of occurrences')\n",
        "            plt.savefig('{}/exp4/freq_dist_f{}'.format(HOME_FOLDER, idx_df+7))   \n",
        "\n",
        "        unique_items_train = list(set(unique_items_train))\n",
        "        for MAX_LENGTH_SEQUENCE in [50, 20, 10, 5]: \n",
        "            sequences_test, sequences_val, sequences_train = [], [], []\n",
        "            for i, usr in enumerate(sequences_test_usr.keys()):\n",
        "                seq_test = sequences_test_usr[usr]\n",
        "                seq_train = sequences_train_usr[usr]\n",
        "                seq_val = sequences_val_usr[usr]\n",
        "\n",
        "                seq_val = np.delete(seq_val, np.where(~np.isin(seq_val[:,1], unique_items_train)), axis=0)\n",
        "                seq_test = np.delete(seq_test, np.where(~np.isin(seq_test[:,1], unique_items_train)), axis=0)\n",
        "                \n",
        "                if len(seq_test) >= MAX_LENGTH_SEQUENCE:\n",
        "                    sequences_test.extend([seq_test[n:n+MAX_LENGTH_SEQUENCE] for n in range(len(seq_test)-MAX_LENGTH_SEQUENCE+1)])\n",
        "                # Val set\n",
        "                if len(seq_val) >= MAX_LENGTH_SEQUENCE:\n",
        "                    sequences_val.extend([seq_val[n:n+MAX_LENGTH_SEQUENCE] for n in range(len(seq_val)-MAX_LENGTH_SEQUENCE+1)])\n",
        "\n",
        "                # Train set\n",
        "                if len(seq_train) >= MAX_LENGTH_SEQUENCE:\n",
        "                    sequences_train.extend([seq_train[n:n+MAX_LENGTH_SEQUENCE] for n in range(len(seq_train)-MAX_LENGTH_SEQUENCE+1)])\n",
        "\n",
        "            # Create and save arrays\n",
        "            sequences_test = np.array(sequences_test)\n",
        "            sequences_train = np.array(sequences_train)\n",
        "            sequences_val = np.array(sequences_val)\n",
        "            FOLDER = '{}/exp4/sequences_{}_len_{}.npy'.format(HOME_FOLDER, idx_df+7, MAX_LENGTH_SEQUENCE)\n",
        "\n",
        "            np.save(FOLDER, np.array([sequences_train, sequences_val, sequences_test]))\n",
        "            print('Shape train sequences: {}, shape val sequences: {}, shape test sequences: {}'.format(sequences_train.shape, sequences_val.shape, sequences_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PrjsaF8zHNC-",
        "colab": {}
      },
      "source": [
        "df = load_raw_dataset(subset_perc=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YkrERlXSHNDB",
        "colab": {}
      },
      "source": [
        "gen_sequences_exp_4(df, visualize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11NGRlBn9zKu",
        "colab_type": "text"
      },
      "source": [
        "### Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blcXNEQn-5CB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Variable settings\n",
        "batch_size = 1000\n",
        "lr = 0.001\n",
        "\n",
        "controller_types = ['gru', 'gru_user']\n",
        "model_types = ['dnc', 'gru']\n",
        "wd = 0\n",
        "wd_user = 0.01\n",
        "gpu = 0\n",
        "clip_controller = 5.0\n",
        "\n",
        "patience_early_stopping = 1\n",
        "optimizer_type = 'adam'\n",
        "embedding_size = 128\n",
        "hidden_size = 128\n",
        "\n",
        "# For sequence lengths:   \n",
        "max_input_count_per_batch = 20 * batch_size\n",
        "\n",
        "# DNC SPECIFIC\n",
        "lr = 0.001\n",
        "layer_sizes = [hidden_size]\n",
        "lstm_use_all_outputs = False\n",
        "\n",
        "mask_min = 0.0\n",
        "grad_clip = 10\n",
        "\n",
        "dealloc_content = True\n",
        "sharpness_control = True\n",
        "masked_lookup = True    \n",
        "return_sequences = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny9HRV_nQguZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_set = '{}/exp4/sequences_{}_len_{}.npy'\n",
        "n_epochs = 25\n",
        "max_input_count_per_batch = 80000\n",
        "\n",
        "for seq_len in [5, 10, 20, 50]:\n",
        "    for model_type in model_types:\n",
        "        print('---------')\n",
        "        print('Using model type {}'.format(model_type))\n",
        "        for user_based in [0, 1]:\n",
        "            print('User based {}'.format(user_based))\n",
        "            for idx_fold in [7, 8, 9]:\n",
        "                file_url = file_set.format(HOME_FOLDER, idx_fold, seq_len)\n",
        "                controller_type = controller_types[user_based]\n",
        "                file_results = '{}/exp4/results_model_type_{}_ub_{}_f{}_len_{}.npy'.format(HOME_FOLDER, model_type, user_based, idx_fold, seq_len)\n",
        "\n",
        "                if os.path.isfile(file_results):\n",
        "                    results = np.load(file_results, allow_pickle=True)\n",
        "                    if len(results) > 1:\n",
        "                        continue\n",
        "                else:\n",
        "                    hyperparams = None\n",
        "                    results = [hyperparams]\n",
        "                model_file = '{}/exp4/models/model_type_{}_ub_{}_f{}_len_{}_hp.pth'.format(HOME_FOLDER, model_type, user_based, idx_fold, seq_len)\n",
        "\n",
        "                n_read_heads, mem_count, data_word_size = 1, 128, 64\n",
        "                print(file_url)\n",
        "                train_loader, val_loader, test_loader, size_voca, size_users = load_data(location_files=file_url)\n",
        "\n",
        "                if not bool(user_based):\n",
        "                    size_users = None\n",
        "                \n",
        "                if hyperparams is None:\n",
        "                    hyperparams = train(n_epochs, size_voca, train_loader, test_loader, val_loader, size_users)\n",
        "                    results.append(hyperparams)\n",
        "                    np.save(file_results, np.array(results))\n",
        "                print(hyperparams)\n",
        "                model_file = '{}/exp4/models/model_type_{}_ub_{}_f{}_len_{}_fin.pth'.format(HOME_FOLDER, model_type, user_based, idx_fold, seq_len)\n",
        "\n",
        "                train_loader, test_loader, size_voca, size_users, items_not_popular, items_semi_popular, items_popular = load_data(location_files=file_url, merge_train_val=True)\n",
        "\n",
        "                if not bool(user_based):\n",
        "                    size_users = None\n",
        "\n",
        "                print('Following hyperparameters were found: {}'.format(hyperparams))\n",
        "\n",
        "                fold_results = train(hyperparams['n_epochs'], size_voca, train_loader, test_loader, val_loader=None, size_users=size_users, items_not_popular=items_not_popular, items_semi_popular=items_semi_popular, items_popular=items_popular)\n",
        "\n",
        "                results.append(fold_results)\n",
        "                np.save(file_results, np.array(results))\n",
        "                print('----------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEretH0LQbM0",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtlFO_uFeNoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "controller_types = ['gru', 'gru_user']\n",
        "model_types = ['dnc', 'gru']\n",
        "results = {}\n",
        "for seq_len in [5, 10, 20, 50]:\n",
        "    for model_type in model_types:\n",
        "        for user_based in [0, 1]:\n",
        "            temp = []\n",
        "            for idx_fold in [7, 8, 9]:\n",
        "                file_results = '{}/exp4/results_model_type_{}_ub_{}_f{}_len_{}.npy'.format(HOME_FOLDER, model_type, user_based, idx_fold, seq_len)\n",
        "                res = np.load(file_results, allow_pickle=True)\n",
        "\n",
        "                temp.append(res[1])\n",
        "            #Recall@1, Recall@20, MRR@20, recall20_p/sp/np, recall_1_p/sp/np, loss\n",
        "            if seq_len == 5:\n",
        "                results['{}_ub_{}_r20'.format(model_type, user_based)] = [np.mean(np.array(temp)[:,0])]\n",
        "                results['{}_ub_{}_r1'.format(model_type, user_based)] = [np.mean(np.array(temp)[:,1])]\n",
        "                \n",
        "                results['{}_ub_{}_r20_p'.format(model_type, user_based)] = [np.mean(np.array(temp)[:,3])]\n",
        "                results['{}_ub_{}_r20_sp'.format(model_type, user_based)] = [np.mean(np.array(temp)[:,4])]\n",
        "                results['{}_ub_{}_r20_np'.format(model_type, user_based)] = [np.mean(np.array(temp)[:,5])]\n",
        "                \n",
        "                results['{}_ub_{}_r1_p'.format(model_type, user_based)] = [np.mean(np.array(temp)[:,6])]\n",
        "                results['{}_ub_{}_r1_sp'.format(model_type, user_based)] = [np.mean(np.array(temp)[:,7])]\n",
        "                results['{}_ub_{}_r1_np'.format(model_type, user_based)] = [np.mean(np.array(temp)[:,8])]\n",
        "            else:\n",
        "                results['{}_ub_{}_r20'.format(model_type, user_based)].append(np.mean(np.array(temp)[:,0]))\n",
        "                results['{}_ub_{}_r1'.format(model_type, user_based)].append(np.mean(np.array(temp)[:,1]))\n",
        "                \n",
        "                results['{}_ub_{}_r20_p'.format(model_type, user_based)].append(np.mean(np.array(temp)[:,3]))\n",
        "                results['{}_ub_{}_r20_sp'.format(model_type, user_based)].append(np.mean(np.array(temp)[:,4]))\n",
        "                results['{}_ub_{}_r20_np'.format(model_type, user_based)].append(np.mean(np.array(temp)[:,5]))\n",
        "                \n",
        "                results['{}_ub_{}_r1_p'.format(model_type, user_based)].append(np.mean(np.array(temp)[:,6]))\n",
        "                results['{}_ub_{}_r1_sp'.format(model_type, user_based)].append(np.mean(np.array(temp)[:,7]))\n",
        "                results['{}_ub_{}_r1_np'.format(model_type, user_based)].append(np.mean(np.array(temp)[:,8]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CJxebDWgHgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exp_4_results(res, visualize=False):\n",
        "    for i, key in enumerate(['r20', 'r20_p', 'r20_sp', 'r20_np', 'r1', 'r1_p', 'r1_sp', 'r1_np']):\n",
        "        seq_lens = [5, 10, 20, 50]#, 50, 100]\n",
        "        if visualize:\n",
        "            fig = plt.figure()\n",
        "            ax = plt.subplot(111)\n",
        "            ax.plot(seq_lens, results['gru_ub_1_{}'.format(key)], label='User-based GRU', marker='o')\n",
        "            ax.plot(seq_lens, results['dnc_ub_1_{}'.format(key)], label='User-based DNC', marker='o')\n",
        "            ax.plot(seq_lens, results['dnc_ub_0_{}'.format(key)], label='Vanilla GRU', marker='o')\n",
        "            ax.plot(seq_lens, results['gru_ub_0_{}'.format(key)], label='Vanilla DNC', marker='o')\n",
        "            # plt.title('Recall@1 for various sequence lengths.')\n",
        "            if i < 4:\n",
        "                plt.ylabel('Recall@20')\n",
        "                plt.ylim(0.2, 0.3)\n",
        "            else:\n",
        "                plt.ylabel('Recall@1')\n",
        "                plt.ylim(0.15, 0.21)\n",
        "\n",
        "            plt.xlabel('Sequence lengths')\n",
        "            #plt.ylim(0.15, 0.2) #0.2, 0.3  0.15 - 0.2\n",
        "            plt.xticks([5, 10, 20, 50])\n",
        "\n",
        "            ticks = [5, 10, 20, 50]\n",
        "            xm = [20, 50]\n",
        "\n",
        "            ax.set_xscale('log', basex=2)\n",
        "            ax.set_xticks(seq_lens)\n",
        "            ax.set_xticks(xm, minor=True)\n",
        "            ax.set_xticklabels(ticks)\n",
        "            ax.set_xticklabels([\"\"]*len(xm), minor=True)\n",
        "\n",
        "            plt.xticks(seq_lens)\n",
        "            ax.legend()\n",
        "            plt.savefig('{}/exp4/{}_plot_exp4.png'.format(HOME_FOLDER, key))\n",
        "        else:\n",
        "            df = pd.DataFrame(columns=['Model type', 'Sequence length 5', 'Sequence length 10', 'Sequence length 20', 'Sequence length 50'])\n",
        "            idx = 0\n",
        "            for model_type in ['gru', 'dnc']:\n",
        "                for ub in [0, 1]:\n",
        "                    if (model_type == 'gru'):\n",
        "                        if bool(ub):\n",
        "                            model_type_text = 'User-based GRU'\n",
        "                        else:\n",
        "                            model_type_text = 'Vanilla GRU'\n",
        "                    else:\n",
        "                        if bool(ub):\n",
        "                            model_type_text = 'User-based DNC'\n",
        "                        else:\n",
        "                            model_type_text = 'Vanilla DNC'\n",
        "                    temp_res = [float(np.round(x, 3)) for x in results['{}_ub_{}_{}'.format(model_type, ub, key)]]\n",
        "                    df.loc[idx] = [model_type_text] + temp_res\n",
        "                    idx +=1\n",
        "\n",
        "            # To latex\n",
        "            print(df.to_latex(index=False))\n",
        "            print('---')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A8qa7Cg2gAlt",
        "colab": {}
      },
      "source": [
        "# Latex Tables\n",
        "exp_4_results(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma9Bw6nTIWvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plots\n",
        "exp_4_results(results, visualize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}